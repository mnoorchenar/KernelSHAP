{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf480ca-4c1d-4eb9-87d9-3f571ed800ed",
   "metadata": {},
   "source": [
    "# Basic Kernel\n",
    "https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/model_agnostic/Simple%20Kernel%20SHAP.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9009932f-e1d0-4b45-8c87-a1756178bbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnoorche\\AppData\\Local\\Temp\\ipykernel_21016\\4159486803.py:72: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  tree_baseline = float(tree_explainer.expected_value)  # Ensure scalar\n",
      "Using 800 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967c75e3898c4c94b24c92351be6b5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>TreeExplainer SHAP</th>\n",
       "      <th>KernelExplainer SHAP</th>\n",
       "      <th>CustomKernel SHAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feature 1</td>\n",
       "      <td>13.4197</td>\n",
       "      <td>13.4311</td>\n",
       "      <td>12.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feature 2</td>\n",
       "      <td>1.7568</td>\n",
       "      <td>1.3560</td>\n",
       "      <td>3.4165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feature 3</td>\n",
       "      <td>1.9600</td>\n",
       "      <td>0.9772</td>\n",
       "      <td>-0.5405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feature 4</td>\n",
       "      <td>75.3594</td>\n",
       "      <td>74.8056</td>\n",
       "      <td>72.5709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feature 5</td>\n",
       "      <td>1.0264</td>\n",
       "      <td>0.5648</td>\n",
       "      <td>-0.5133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Feature 6</td>\n",
       "      <td>-3.2434</td>\n",
       "      <td>-3.4140</td>\n",
       "      <td>-3.5330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Feature 7</td>\n",
       "      <td>-30.7180</td>\n",
       "      <td>-31.2188</td>\n",
       "      <td>-33.8595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Feature 8</td>\n",
       "      <td>1.8544</td>\n",
       "      <td>1.8552</td>\n",
       "      <td>1.8361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Feature 9</td>\n",
       "      <td>1.4149</td>\n",
       "      <td>0.8683</td>\n",
       "      <td>0.1188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Feature 10</td>\n",
       "      <td>-71.4951</td>\n",
       "      <td>-68.8819</td>\n",
       "      <td>-69.9190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>-1.5242</td>\n",
       "      <td>-0.5323</td>\n",
       "      <td>7.8329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum</th>\n",
       "      <td>Sum</td>\n",
       "      <td>-10.1890</td>\n",
       "      <td>-10.1890</td>\n",
       "      <td>-10.1883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computation Time (s)</th>\n",
       "      <td>Computation Time (s)</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>12.2340</td>\n",
       "      <td>0.0350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Feature  TreeExplainer SHAP  \\\n",
       "0                                Feature 1             13.4197   \n",
       "1                                Feature 2              1.7568   \n",
       "2                                Feature 3              1.9600   \n",
       "3                                Feature 4             75.3594   \n",
       "4                                Feature 5              1.0264   \n",
       "5                                Feature 6             -3.2434   \n",
       "6                                Feature 7            -30.7180   \n",
       "7                                Feature 8              1.8544   \n",
       "8                                Feature 9              1.4149   \n",
       "9                               Feature 10            -71.4951   \n",
       "Baseline                          Baseline             -1.5242   \n",
       "Sum                                    Sum            -10.1890   \n",
       "Computation Time (s)  Computation Time (s)              0.0230   \n",
       "\n",
       "                      KernelExplainer SHAP  CustomKernel SHAP  \n",
       "0                                  13.4311            12.4019  \n",
       "1                                   1.3560             3.4165  \n",
       "2                                   0.9772            -0.5405  \n",
       "3                                  74.8056            72.5709  \n",
       "4                                   0.5648            -0.5133  \n",
       "5                                  -3.4140            -3.5330  \n",
       "6                                 -31.2188           -33.8595  \n",
       "7                                   1.8552             1.8361  \n",
       "8                                   0.8683             0.1188  \n",
       "9                                 -68.8819           -69.9190  \n",
       "Baseline                           -0.5323             7.8329  \n",
       "Sum                               -10.1890           -10.1883  \n",
       "Computation Time (s)               12.2340             0.0350  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "import scipy.special\n",
    "import time\n",
    "import shap\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Generate a simple regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Train a RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Select the first test instance for explanation\n",
    "test_instance = X_test[0]\n",
    "\n",
    "# Define helper functions for Kernel SHAP\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(len(s) + 1))\n",
    "\n",
    "def shapley_kernel(M, s):\n",
    "    if s == 0 or s == M:\n",
    "        return 10000  # Large constant for numerical stability\n",
    "    return (M - 1) / (scipy.special.binom(M, s) * s * (M - s))\n",
    "\n",
    "def kernel_shap(f, x, reference, M):\n",
    "    X = np.zeros((2**M, M + 1))\n",
    "    X[:, -1] = 1\n",
    "    weights = np.zeros(2**M)\n",
    "    V = np.zeros((2**M, M))\n",
    "    for i in range(2**M):\n",
    "        V[i, :] = reference\n",
    "\n",
    "    for i, s in enumerate(powerset(range(M))):\n",
    "        s = list(s)\n",
    "        V[i, s] = x[s]\n",
    "        X[i, s] = 1\n",
    "        weights[i] = shapley_kernel(M, len(s))\n",
    "    y = f(V)\n",
    "    wsq = np.sqrt(weights)\n",
    "    result = np.linalg.lstsq(wsq[:, None] * X, wsq * y, rcond=None)[0]\n",
    "    return result\n",
    "\n",
    "# Define the prediction function\n",
    "def prediction_function(X):\n",
    "    return regressor.predict(X)\n",
    "\n",
    "# Define the reference input (mean of the training data)\n",
    "reference = np.mean(X_train, axis=0)\n",
    "\n",
    "# Number of features\n",
    "M = X_train.shape[1]\n",
    "\n",
    "# Step 6: Compute SHAP values and time the computation\n",
    "\n",
    "# TreeExplainer SHAP\n",
    "start_time = time.time()\n",
    "tree_explainer = shap.TreeExplainer(regressor)\n",
    "tree_shap_values = tree_explainer.shap_values(test_instance.reshape(1, -1))[0]\n",
    "tree_baseline = float(tree_explainer.expected_value)  # Ensure scalar\n",
    "tree_time = time.time() - start_time\n",
    "\n",
    "# KernelExplainer SHAP\n",
    "start_time = time.time()\n",
    "kernel_explainer = shap.KernelExplainer(prediction_function, X_train)\n",
    "kernel_shap_values = kernel_explainer.shap_values(test_instance.reshape(1, -1))[0]\n",
    "kernel_baseline = float(kernel_explainer.expected_value)  # Ensure scalar\n",
    "kernel_time = time.time() - start_time\n",
    "\n",
    "# Custom Kernel SHAP\n",
    "start_time = time.time()\n",
    "custom_kernel_phi = kernel_shap(prediction_function, test_instance, reference, M)\n",
    "custom_baseline = float(custom_kernel_phi[-1])  # Baseline (intercept), ensure scalar\n",
    "custom_kernel_shap_values = custom_kernel_phi[:-1]  # SHAP values for features\n",
    "custom_kernel_time = time.time() - start_time  # Time taken for Custom Kernel SHAP\n",
    "\n",
    "# Step 7: Display Results\n",
    "results = pd.DataFrame({\n",
    "    \"Feature\": [f\"Feature {i+1}\" for i in range(X.shape[1])],\n",
    "    \"TreeExplainer SHAP\": np.round(tree_shap_values, 4),  # TreeExplainer output\n",
    "    \"KernelExplainer SHAP\": np.round(kernel_shap_values, 4),  # KernelExplainer output\n",
    "    \"CustomKernel SHAP\": np.round(custom_kernel_shap_values, 4),  # Custom Kernel SHAP\n",
    "})\n",
    "\n",
    "# Add baselines to the results\n",
    "results.loc[\"Baseline\"] = {\n",
    "    \"Feature\": \"Baseline\",\n",
    "    \"TreeExplainer SHAP\": np.round(tree_baseline, 4),\n",
    "    \"KernelExplainer SHAP\": np.round(kernel_baseline, 4),\n",
    "    \"CustomKernel SHAP\": np.round(custom_baseline, 4),\n",
    "}\n",
    "\n",
    "# Calculate the sum of SHAP values + baseline for each method\n",
    "tree_shap_sum = float(np.sum(tree_shap_values) + tree_baseline)  # Ensure scalar\n",
    "kernel_shap_sum = float(np.sum(kernel_shap_values) + kernel_baseline)  # Ensure scalar\n",
    "custom_shap_sum = float(np.sum(custom_kernel_shap_values) + custom_baseline)  # Ensure scalar\n",
    "\n",
    "results.loc[\"Sum\"] = {\n",
    "    \"Feature\": \"Sum\",\n",
    "    \"TreeExplainer SHAP\": np.round(tree_shap_sum, 4),\n",
    "    \"KernelExplainer SHAP\": np.round(kernel_shap_sum, 4),\n",
    "    \"CustomKernel SHAP\": np.round(custom_shap_sum, 4),\n",
    "}\n",
    "\n",
    "# Add computation times as the last row\n",
    "results.loc[\"Computation Time (s)\"] = {\n",
    "    \"Feature\": \"Computation Time (s)\",\n",
    "    \"TreeExplainer SHAP\": np.round(tree_time, 4),\n",
    "    \"KernelExplainer SHAP\": np.round(kernel_time, 4),\n",
    "    \"CustomKernel SHAP\": np.round(custom_kernel_time, 4),\n",
    "}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd34993-d749-42ba-8e2e-6a4227c81272",
   "metadata": {},
   "source": [
    "# Updated Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ac7484cc-971f-4e3e-b6b1-f203aa88b6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnoorche\\AppData\\Local\\Temp\\ipykernel_21016\\1762167479.py:117: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  tree_baseline = float(tree_explainer.expected_value)  # Ensure scalar\n",
      "Using 500 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9775775e51764a468fb025260b8a90e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>TreeExplainer SHAP</th>\n",
       "      <th>KernelExplainer SHAP</th>\n",
       "      <th>CustomKernel SHAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feature 1</td>\n",
       "      <td>-47.8522</td>\n",
       "      <td>-50.7102</td>\n",
       "      <td>-49.7264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feature 2</td>\n",
       "      <td>9.0346</td>\n",
       "      <td>5.7863</td>\n",
       "      <td>9.8388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feature 3</td>\n",
       "      <td>3.3038</td>\n",
       "      <td>-2.5937</td>\n",
       "      <td>0.9901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feature 4</td>\n",
       "      <td>-2.1609</td>\n",
       "      <td>-4.1729</td>\n",
       "      <td>-17.3094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feature 5</td>\n",
       "      <td>-0.7324</td>\n",
       "      <td>1.7503</td>\n",
       "      <td>-0.6865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Feature 6</td>\n",
       "      <td>-1.6299</td>\n",
       "      <td>-1.6725</td>\n",
       "      <td>4.1388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Feature 7</td>\n",
       "      <td>25.5404</td>\n",
       "      <td>14.7520</td>\n",
       "      <td>4.7303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Feature 8</td>\n",
       "      <td>35.0099</td>\n",
       "      <td>29.4719</td>\n",
       "      <td>30.4919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Feature 9</td>\n",
       "      <td>-74.1182</td>\n",
       "      <td>-83.5356</td>\n",
       "      <td>-88.8265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Feature 10</td>\n",
       "      <td>-115.6296</td>\n",
       "      <td>-82.9742</td>\n",
       "      <td>-79.2923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>1.3357</td>\n",
       "      <td>5.9997</td>\n",
       "      <td>22.7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sum</td>\n",
       "      <td>-167.8988</td>\n",
       "      <td>-167.8988</td>\n",
       "      <td>-162.8872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Computation Time (s)</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>6.5550</td>\n",
       "      <td>0.0130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Feature  TreeExplainer SHAP  KernelExplainer SHAP  \\\n",
       "0             Feature 1            -47.8522              -50.7102   \n",
       "1             Feature 2              9.0346                5.7863   \n",
       "2             Feature 3              3.3038               -2.5937   \n",
       "3             Feature 4             -2.1609               -4.1729   \n",
       "4             Feature 5             -0.7324                1.7503   \n",
       "5             Feature 6             -1.6299               -1.6725   \n",
       "6             Feature 7             25.5404               14.7520   \n",
       "7             Feature 8             35.0099               29.4719   \n",
       "8             Feature 9            -74.1182              -83.5356   \n",
       "9            Feature 10           -115.6296              -82.9742   \n",
       "0              Baseline              1.3357                5.9997   \n",
       "1                   Sum           -167.8988             -167.8988   \n",
       "2  Computation Time (s)              0.0260                6.5550   \n",
       "\n",
       "   CustomKernel SHAP  \n",
       "0           -49.7264  \n",
       "1             9.8388  \n",
       "2             0.9901  \n",
       "3           -17.3094  \n",
       "4            -0.6865  \n",
       "5             4.1388  \n",
       "6             4.7303  \n",
       "7            30.4919  \n",
       "8           -88.8265  \n",
       "9           -79.2923  \n",
       "0            22.7639  \n",
       "1          -162.8872  \n",
       "2             0.0130  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "import scipy.special\n",
    "import time\n",
    "import shap\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Generate a simple regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Train a RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Select the first test instance for explanation\n",
    "test_instance = X_test[0]\n",
    "\n",
    "# Monte Carlo-based sampling for subsets\n",
    "def sample_subsets(M, num_samples):\n",
    "    \"\"\"\n",
    "    Generate random subsets of features using Monte Carlo sampling.\n",
    "    This replaces the exhaustive powerset generation for efficiency.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    subsets = []\n",
    "    for _ in range(num_samples):\n",
    "        subset = np.random.choice([0, 1], size=M, p=[0.5, 0.5])\n",
    "        subsets.append(np.where(subset == 1)[0])  # Indices of features in the subset\n",
    "    return subsets\n",
    "\n",
    "# Shapley kernel weight calculation\n",
    "def shapley_kernel(M, s):\n",
    "    \"\"\"\n",
    "    Compute the Shapley kernel weight for a given subset size `s`.\n",
    "    \"\"\"\n",
    "    if s == 0 or s == M:\n",
    "        return 10000  # Large constant for numerical stability\n",
    "    return (M - 1) / (scipy.special.binom(M, s) * s * (M - s))\n",
    "\n",
    "# Kernel SHAP implementation with optional Monte Carlo sampling\n",
    "def kernel_shap(f, x, reference, M, num_samples=None):\n",
    "    \"\"\"\n",
    "    Compute SHAP values using the Kernel SHAP method.\n",
    "    - If `num_samples` is None, compute exact SHAP values (2^M subsets).\n",
    "    - Otherwise, approximate SHAP values using Monte Carlo sampling.\n",
    "\n",
    "    Args:\n",
    "        f (callable): Prediction function.\n",
    "        x (np.ndarray): Input instance to explain.\n",
    "        reference (np.ndarray): Reference values for each feature.\n",
    "        M (int): Number of features.\n",
    "        num_samples (int): Number of Monte Carlo samples (optional).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: SHAP values for the input instance.\n",
    "    \"\"\"\n",
    "    if num_samples is None:\n",
    "        # Use exact computation (exhaustive powerset)\n",
    "        subsets = list(itertools.chain.from_iterable(itertools.combinations(range(M), r) for r in range(M + 1)))\n",
    "    else:\n",
    "        # Use Monte Carlo sampling\n",
    "        subsets = sample_subsets(M, num_samples)\n",
    "\n",
    "    # Prepare matrices for least-squares regression\n",
    "    num_subsets = len(subsets)\n",
    "    X = np.zeros((num_subsets, M + 1))\n",
    "    X[:, -1] = 1  # Bias term\n",
    "    V = np.tile(reference, (num_subsets, 1))\n",
    "    weights = np.zeros(num_subsets)\n",
    "\n",
    "    # Populate X, V, and weights based on subsets\n",
    "    for i, s in enumerate(subsets):\n",
    "        s = list(s)\n",
    "        V[i, s] = x[s]\n",
    "        X[i, s] = 1\n",
    "        weights[i] = shapley_kernel(M, len(s))\n",
    "\n",
    "    # Normalize weights for numerical stability\n",
    "    weights /= np.sum(weights)\n",
    "\n",
    "    # Model predictions for perturbed inputs\n",
    "    y = f(V)\n",
    "\n",
    "    # Weighted least-squares regression\n",
    "    wsq = np.sqrt(weights)\n",
    "    result = np.linalg.lstsq(wsq[:, None] * X, wsq * y, rcond=None)[0]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Define the prediction function\n",
    "def prediction_function(X):\n",
    "    return regressor.predict(X)\n",
    "\n",
    "# Define the reference input (mean of the training data)\n",
    "reference = np.mean(X_train, axis=0)\n",
    "\n",
    "# Number of features\n",
    "M = X_train.shape[1]\n",
    "\n",
    "# Step 6: Compute SHAP values and time the computation\n",
    "\n",
    "# TreeExplainer SHAP\n",
    "start_time = time.time()\n",
    "tree_explainer = shap.TreeExplainer(regressor)\n",
    "tree_shap_values = tree_explainer.shap_values(test_instance.reshape(1, -1))[0]\n",
    "tree_baseline = float(tree_explainer.expected_value)  # Ensure scalar\n",
    "tree_time = time.time() - start_time\n",
    "\n",
    "# KernelExplainer SHAP\n",
    "start_time = time.time()\n",
    "kernel_explainer = shap.KernelExplainer(prediction_function, np.tile(reference, (500, 1)))\n",
    "kernel_shap_values = kernel_explainer.shap_values(test_instance.reshape(1, -1))[0]\n",
    "kernel_baseline = float(kernel_explainer.expected_value)  # Ensure scalar\n",
    "kernel_time = time.time() - start_time\n",
    "\n",
    "# Custom Kernel SHAP\n",
    "start_time = time.time()\n",
    "num_samples = 100  # Increase Monte Carlo samples for accuracy\n",
    "custom_kernel_phi = kernel_shap(prediction_function, test_instance, reference, M, num_samples)\n",
    "custom_baseline = float(custom_kernel_phi[-1])  # Baseline (intercept), ensure scalar\n",
    "custom_kernel_shap_values = custom_kernel_phi[:-1]  # SHAP values for features\n",
    "custom_kernel_time = time.time() - start_time  # Time taken for Custom Kernel SHAP\n",
    "\n",
    "# Step 7: Display Results\n",
    "results = pd.DataFrame({\n",
    "    \"Feature\": [f\"Feature {i+1}\" for i in range(X.shape[1])],\n",
    "    \"TreeExplainer SHAP\": np.round(tree_shap_values, 4),  # TreeExplainer output\n",
    "    \"KernelExplainer SHAP\": np.round(kernel_shap_values, 4),  # KernelExplainer output\n",
    "    \"CustomKernel SHAP\": np.round(custom_kernel_shap_values, 4),  # Custom Kernel SHAP\n",
    "})\n",
    "\n",
    "# Add baselines to the results\n",
    "results = pd.concat([\n",
    "    results,\n",
    "    pd.DataFrame([\n",
    "        {\"Feature\": \"Baseline\", \"TreeExplainer SHAP\": np.round(tree_baseline, 4),\n",
    "         \"KernelExplainer SHAP\": np.round(kernel_baseline, 4),\n",
    "         \"CustomKernel SHAP\": np.round(custom_baseline, 4)},\n",
    "        {\"Feature\": \"Sum\", \"TreeExplainer SHAP\": np.round(np.sum(tree_shap_values) + tree_baseline, 4),\n",
    "         \"KernelExplainer SHAP\": np.round(np.sum(kernel_shap_values) + kernel_baseline, 4),\n",
    "         \"CustomKernel SHAP\": np.round(np.sum(custom_kernel_shap_values) + custom_baseline, 4)},\n",
    "        {\"Feature\": \"Computation Time (s)\", \"TreeExplainer SHAP\": np.round(tree_time, 4),\n",
    "         \"KernelExplainer SHAP\": np.round(kernel_time, 4),\n",
    "         \"CustomKernel SHAP\": np.round(custom_kernel_time, 4)},\n",
    "    ])\n",
    "])\n",
    "\n",
    "# Print the results\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b3f9cc-3c92-4e9f-84f6-6979219c0ce5",
   "metadata": {},
   "source": [
    "# mean with sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5228db5e-4cfd-4b56-8277-92ae8ced5e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a603dd73804eceb9f9cb936f22657b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>TreeExplainer SHAP</th>\n",
       "      <th>KernelExplainer SHAP</th>\n",
       "      <th>MeanKernel SHAP (auto)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feature 1</td>\n",
       "      <td>21.0811</td>\n",
       "      <td>10.7461</td>\n",
       "      <td>24.1879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feature 2</td>\n",
       "      <td>-7.5035</td>\n",
       "      <td>-8.2388</td>\n",
       "      <td>-15.5479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feature 3</td>\n",
       "      <td>1.8309</td>\n",
       "      <td>2.0683</td>\n",
       "      <td>1.9465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feature 4</td>\n",
       "      <td>-88.9366</td>\n",
       "      <td>-91.7532</td>\n",
       "      <td>-72.6232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feature 5</td>\n",
       "      <td>-33.2918</td>\n",
       "      <td>-34.7344</td>\n",
       "      <td>-42.7720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>8.7755</td>\n",
       "      <td>23.8676</td>\n",
       "      <td>6.7641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sum</td>\n",
       "      <td>-98.0444</td>\n",
       "      <td>-98.0444</td>\n",
       "      <td>-98.0446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Computation Time (s)</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>0.0140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prediction (Test Instance)</td>\n",
       "      <td>-98.0444</td>\n",
       "      <td>-98.0444</td>\n",
       "      <td>-98.0444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Feature  TreeExplainer SHAP  KernelExplainer SHAP  \\\n",
       "0                   Feature 1             21.0811               10.7461   \n",
       "1                   Feature 2             -7.5035               -8.2388   \n",
       "2                   Feature 3              1.8309                2.0683   \n",
       "3                   Feature 4            -88.9366              -91.7532   \n",
       "4                   Feature 5            -33.2918              -34.7344   \n",
       "0                    Baseline              8.7755               23.8676   \n",
       "1                         Sum            -98.0444              -98.0444   \n",
       "2        Computation Time (s)              0.0350                0.0450   \n",
       "3  Prediction (Test Instance)            -98.0444              -98.0444   \n",
       "\n",
       "   MeanKernel SHAP (auto)  \n",
       "0                 24.1879  \n",
       "1                -15.5479  \n",
       "2                  1.9465  \n",
       "3                -72.6232  \n",
       "4                -42.7720  \n",
       "0                  6.7641  \n",
       "1                -98.0446  \n",
       "2                  0.0140  \n",
       "3                -98.0444  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "import scipy.special\n",
    "import time\n",
    "import shap\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Step 1: Generate a simple regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=5, noise=0.5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Train a RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Select the first test instance for explanation\n",
    "test_instance = X_test[0]\n",
    "\n",
    "# Shapley kernel weight calculation\n",
    "def shapley_kernel(M, s):\n",
    "    \"\"\"\n",
    "    Calculate the Shapley kernel weight for a subset of size s with M total features.\n",
    "\n",
    "    Parameters:\n",
    "    - M: Total number of features.\n",
    "    - s: Size of the subset.\n",
    "\n",
    "    Returns:\n",
    "    - The kernel weight for the subset size.\n",
    "    \"\"\"\n",
    "    if s == 0 or s == M:\n",
    "        return 10000  # Large constant for numerical stability\n",
    "    return (M - 1) / (scipy.special.binom(M, s) * s * (M - s))\n",
    "\n",
    "# Mean Kernel SHAP function (Exact computation)\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "def mean_kernel_shap_with_constraint(f, x, reference, M, nsamples=\"auto\"):\n",
    "    r\"\"\"\n",
    "    Kernel SHAP with additive efficiency constraint and kernel weights.\n",
    "    \n",
    "    Parameters:\n",
    "    - f: The model function to explain.\n",
    "    - x: Instance to explain (1D array of feature values).\n",
    "    - reference: Reference value for each feature (1D array, usually the mean of the dataset).\n",
    "    - M: Number of features.\n",
    "    - nsamples: Number of samples for the sampling method. \n",
    "                If \"auto\", uses nsamples = min(2 * M + 2048, 2^M).\n",
    "    \n",
    "    Returns:\n",
    "    - shap_values: Shapley values (1D array of size M).\n",
    "    - baseline: The baseline value (\\phi_0).\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Calculate the number of samples\n",
    "    if nsamples == \"auto\":\n",
    "        nsamples = min(2 * M + 2048, 2 ** M)\n",
    "\n",
    "    # Step 2: Sampling subsets with Shapley kernel weights\n",
    "    np.random.seed(0)\n",
    "    subsets = []  # List to store sampled subsets\n",
    "    weights = []  # List to store corresponding weights\n",
    "\n",
    "    for _ in range(nsamples):\n",
    "        # Generate a random binary mask representing a subset\n",
    "        subset = np.random.choice([0, 1], size=M, p=[0.5, 0.5])  # Random binary mask\n",
    "        subset_indices = np.where(subset == 1)[0]\n",
    "        subsets.append(subset_indices)\n",
    "        \n",
    "        # Compute Shapley kernel weight for the subset\n",
    "        s_size = len(subset_indices)\n",
    "        if s_size == 0 or s_size == M:\n",
    "            weight = 10000  # Large constant for stability\n",
    "        else:\n",
    "            weight = (M - 1) / (scipy.special.binom(M, s_size) * s_size * (M - s_size))\n",
    "        weights.append(weight)\n",
    "\n",
    "    # Normalize weights\n",
    "    weights = np.array(weights)\n",
    "    weights /= np.sum(weights)\n",
    "\n",
    "    # Number of subsets\n",
    "    num_subsets = len(subsets)\n",
    "\n",
    "    # Step 3: Initialize matrices for regression\n",
    "    X = np.zeros((num_subsets, M))  # Design matrix\n",
    "    V = np.zeros((num_subsets, M))  # Perturbed input data matrix\n",
    "\n",
    "    # Prepare feature subsets\n",
    "    for i, s in enumerate(subsets):\n",
    "        V[i, :] = reference  # Start with reference values\n",
    "        V[i, s] = x[s]       # Replace selected features with values from `x`\n",
    "        X[i, s] = 1          # Set feature presence in the design matrix\n",
    "\n",
    "    # Ensure `reference` and `x` are reshaped to (1, M) before passing to `f`\n",
    "    reference = reference.reshape(1, -1)\n",
    "    x = x.reshape(1, -1)\n",
    "\n",
    "    # Step 4: Evaluate the model on the sampled feature subsets\n",
    "    y = f(V) - f(reference)  # Centered outputs: v_x(s) - v_x(0)\n",
    "\n",
    "    # Step 5: Add efficiency constraint row to X and y\n",
    "    # Add a row to enforce the constraint: sum(ϕ_i) = v_x(1) - v_x(0)\n",
    "    efficiency_row = np.ones((1, M))  # Row of ones\n",
    "    X = np.vstack([X, efficiency_row])  # Append to the design matrix\n",
    "    y = np.append(y, f(x) - f(reference))  # Append the efficiency constraint to outputs\n",
    "\n",
    "    # Add corresponding weight for the efficiency constraint\n",
    "    weights = np.append(weights, 1.0)  # Assign unit weight to the efficiency constraint\n",
    "\n",
    "    # Step 6: Weighted least squares regression\n",
    "    # Compute weighted least squares: Minimize Σ (w_i * (y_i - X_i^T * φ)^2)\n",
    "    wsq = np.sqrt(weights)  # Square root of weights\n",
    "    result = np.linalg.lstsq(wsq[:, None] * X, wsq * y, rcond=None)[0]  # Solve for SHAP values\n",
    "\n",
    "    # Step 7: Return results\n",
    "    return result, f(reference).flatten()\n",
    "\n",
    "\n",
    "# Define the prediction function\n",
    "def prediction_function(X):\n",
    "    return regressor.predict(X)\n",
    "\n",
    "# Number of features\n",
    "M = X_train.shape[1]\n",
    "\n",
    "# Step 6: Compute SHAP values and time the computation\n",
    "\n",
    "# TreeExplainer SHAP\n",
    "start_time = time.time()\n",
    "tree_explainer = shap.TreeExplainer(regressor)\n",
    "tree_shap_values = tree_explainer.shap_values(test_instance.reshape(1, -1))[0]\n",
    "tree_baseline = float(tree_explainer.expected_value[0])  # Extract scalar from array\n",
    "tree_time = time.time() - start_time\n",
    "\n",
    "# KernelExplainer SHAP\n",
    "start_time = time.time()\n",
    "background = shap.sample(X_train, 100)  # Summarize the background to 100 samples\n",
    "kernel_explainer = shap.KernelExplainer(prediction_function, background)\n",
    "kernel_shap_values = kernel_explainer.shap_values(test_instance.reshape(1, -1))[0]\n",
    "kernel_baseline = float(kernel_explainer.expected_value)  # Ensure scalar\n",
    "kernel_time = time.time() - start_time\n",
    "\n",
    "# Mean Kernel SHAP (Auto)\n",
    "start_time = time.time()\n",
    "reference = np.mean(X_train, axis=0)  # Mean feature values\n",
    "mean_kernel_phi, mean_baseline = mean_kernel_shap_with_constraint(prediction_function, test_instance, reference, M)\n",
    "mean_kernel_shap_values = mean_kernel_phi  # SHAP values for features\n",
    "mean_kernel_time = time.time() - start_time\n",
    "\n",
    "# Compute the prediction of the test_instance\n",
    "test_instance_prediction = prediction_function(test_instance.reshape(1, -1))[0]\n",
    "\n",
    "# Step 6: Display Results\n",
    "results = pd.DataFrame({\n",
    "    \"Feature\": [f\"Feature {i+1}\" for i in range(X.shape[1])],\n",
    "    \"TreeExplainer SHAP\": np.round(tree_shap_values, 4),\n",
    "    \"KernelExplainer SHAP\": np.round(kernel_shap_values, 4),\n",
    "    \"MeanKernel SHAP (auto)\": np.round(mean_kernel_shap_values, 4),\n",
    "})\n",
    "\n",
    "# Append baseline, sum, computation time, and prediction\n",
    "results = pd.concat([\n",
    "    results,\n",
    "    pd.DataFrame([\n",
    "        {\"Feature\": \"Baseline\", \"TreeExplainer SHAP\": np.round(tree_baseline, 4),\n",
    "         \"KernelExplainer SHAP\": np.round(kernel_baseline, 4),\n",
    "         \"MeanKernel SHAP (auto)\": np.round(mean_baseline[0], 4)},\n",
    "        {\"Feature\": \"Sum\", \"TreeExplainer SHAP\": np.round(np.sum(tree_shap_values) + tree_baseline, 4),\n",
    "         \"KernelExplainer SHAP\": np.round(np.sum(kernel_shap_values) + kernel_baseline, 4),\n",
    "         \"MeanKernel SHAP (auto)\": np.round(np.sum(mean_kernel_shap_values) + mean_baseline[0], 4)},\n",
    "        {\"Feature\": \"Computation Time (s)\", \"TreeExplainer SHAP\": np.round(tree_time, 4),\n",
    "         \"KernelExplainer SHAP\": np.round(kernel_time, 4),\n",
    "         \"MeanKernel SHAP (auto)\": np.round(mean_kernel_time, 4)},\n",
    "        {\"Feature\": \"Prediction (Test Instance)\", \"TreeExplainer SHAP\": np.round(test_instance_prediction, 4),\n",
    "         \"KernelExplainer SHAP\": np.round(test_instance_prediction, 4), \"MeanKernel SHAP (auto)\": np.round(test_instance_prediction, 4)}\n",
    "    ])\n",
    "])\n",
    "\n",
    "\n",
    "# Print the results\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a33a74-a698-4fd4-96a5-34c7a5a2d935",
   "metadata": {},
   "source": [
    "# kernel based on game theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ff7a944d-c70d-468a-9d70-ef496840ed72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>TreeExplainer SHAP</th>\n",
       "      <th>KernelExplainer SHAP</th>\n",
       "      <th>MeanKernel SHAP (auto)</th>\n",
       "      <th>Weighted SHAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feature 1</td>\n",
       "      <td>21.0811</td>\n",
       "      <td>10.7461</td>\n",
       "      <td>24.1879</td>\n",
       "      <td>19.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feature 2</td>\n",
       "      <td>-7.5035</td>\n",
       "      <td>-8.2388</td>\n",
       "      <td>-15.5479</td>\n",
       "      <td>-8.0885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feature 3</td>\n",
       "      <td>1.8309</td>\n",
       "      <td>2.0683</td>\n",
       "      <td>1.9465</td>\n",
       "      <td>1.4556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feature 4</td>\n",
       "      <td>-88.9366</td>\n",
       "      <td>-91.7532</td>\n",
       "      <td>-72.6232</td>\n",
       "      <td>-88.2724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feature 5</td>\n",
       "      <td>-33.2918</td>\n",
       "      <td>-34.7344</td>\n",
       "      <td>-42.7720</td>\n",
       "      <td>-31.5424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>8.7755</td>\n",
       "      <td>23.8676</td>\n",
       "      <td>6.7641</td>\n",
       "      <td>9.2730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sum</td>\n",
       "      <td>-98.0444</td>\n",
       "      <td>-98.0444</td>\n",
       "      <td>-98.0446</td>\n",
       "      <td>-98.0444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Computation Time (s)</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prediction (Test Instance)</td>\n",
       "      <td>-98.0444</td>\n",
       "      <td>-98.0444</td>\n",
       "      <td>-98.0444</td>\n",
       "      <td>-98.0444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Feature  TreeExplainer SHAP  KernelExplainer SHAP  \\\n",
       "0                   Feature 1             21.0811               10.7461   \n",
       "1                   Feature 2             -7.5035               -8.2388   \n",
       "2                   Feature 3              1.8309                2.0683   \n",
       "3                   Feature 4            -88.9366              -91.7532   \n",
       "4                   Feature 5            -33.2918              -34.7344   \n",
       "0                    Baseline              8.7755               23.8676   \n",
       "1                         Sum            -98.0444              -98.0444   \n",
       "2        Computation Time (s)              0.0350                0.0450   \n",
       "3  Prediction (Test Instance)            -98.0444              -98.0444   \n",
       "\n",
       "   MeanKernel SHAP (auto)  Weighted SHAP  \n",
       "0                 24.1879        19.1302  \n",
       "1                -15.5479        -8.0885  \n",
       "2                  1.9465         1.4556  \n",
       "3                -72.6232       -88.2724  \n",
       "4                -42.7720       -31.5424  \n",
       "0                  6.7641         9.2730  \n",
       "1                -98.0446       -98.0444  \n",
       "2                  0.0140         0.0830  \n",
       "3                -98.0444       -98.0444  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy.stats import mode\n",
    "\n",
    "def shapley_kernel_weight(num_features, subset_size, normalize=True):\n",
    "    \"\"\"\n",
    "    Calculate the Shapley kernel weight for a subset of a given size.\n",
    "\n",
    "    Args:\n",
    "        num_features (int): Total number of features.\n",
    "        subset_size (int): Number of features in the subset.\n",
    "        normalize (bool): Whether to normalize the weight to sum to 1.\n",
    "\n",
    "    Returns:\n",
    "        float: Weight for the subset size.\n",
    "    \"\"\"\n",
    "    # Ensure subset size is valid\n",
    "    if subset_size == 0 or subset_size == num_features:\n",
    "        return 0\n",
    "\n",
    "    # Original weight calculation\n",
    "    weight = (num_features - 1) / (math.comb(num_features, subset_size) * subset_size * (num_features - subset_size))\n",
    "\n",
    "    # Regularize weight to emphasize intermediate subset sizes\n",
    "    intermediate_factor = 1 / (1 + abs(num_features / 2 - subset_size))\n",
    "    weight *= intermediate_factor\n",
    "\n",
    "    # Normalize weight if specified\n",
    "    if normalize:\n",
    "        total_combinations = 2 ** num_features - 2  # Exclude empty set and full set\n",
    "        weight /= total_combinations\n",
    "\n",
    "    return weight\n",
    "\n",
    "\n",
    "def sample_and_weight(X, num_samples=\"auto\", normalize=False, weighted_sampling=True):\n",
    "    \"\"\"\n",
    "    Sample subsets and compute Shapley kernel weights.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input data, shape (n_samples, n_features).\n",
    "        num_samples (int or str): Number of subsets to sample. Use \"auto\" to calculate based on the formula.\n",
    "        normalize (bool): Whether to normalize the weights.\n",
    "        weighted_sampling (bool): Whether to sample subsets based on weights.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Matrix of sampled subsets (binary masks).\n",
    "        np.ndarray: Array of corresponding weights.\n",
    "    \"\"\"\n",
    "    num_features = X.shape[1]\n",
    "\n",
    "    # Set default number of samples\n",
    "    if num_samples == \"auto\":\n",
    "        num_samples = 2 * num_features + 2048\n",
    "\n",
    "    subsets = []\n",
    "    weights = []\n",
    "\n",
    "    # Precompute weights for all subset sizes if using weighted sampling\n",
    "    if weighted_sampling:\n",
    "        subset_weights = [shapley_kernel_weight(num_features, size, normalize) for size in range(1, num_features)]\n",
    "        cumulative_weights = np.cumsum(subset_weights)\n",
    "        cumulative_weights /= cumulative_weights[-1]  # Normalize to sum to 1\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        if weighted_sampling:\n",
    "            # Sample subset size based on weights\n",
    "            rand = np.random.random()\n",
    "            subset_size = np.searchsorted(cumulative_weights, rand) + 1\n",
    "        else:\n",
    "            # Uniformly sample subset size\n",
    "            subset_size = np.random.randint(1, num_features)\n",
    "\n",
    "        # Generate a binary mask for the subset\n",
    "        subset = np.zeros(num_features, dtype=int)\n",
    "        subset[:subset_size] = 1\n",
    "        np.random.shuffle(subset)\n",
    "\n",
    "        # Compute the Shapley kernel weight\n",
    "        weight = shapley_kernel_weight(num_features, subset_size, normalize)\n",
    "\n",
    "        subsets.append(subset)\n",
    "        weights.append(weight)\n",
    "\n",
    "    weights = np.array(weights)\n",
    "    if normalize:\n",
    "        weights /= np.sum(weights)  # Ensure weights sum to 1\n",
    "\n",
    "    return np.array(subsets), weights\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data_subsets(sample_row, background_data, subsets, use_random=False):\n",
    "    \"\"\"\n",
    "    Generate subsets with data substitution based on subsets and background data using matrix operations.\n",
    "    A feature is treated as categorical if it has only 2 unique values (e.g., {0, 1}).\n",
    "\n",
    "    Args:\n",
    "        sample_row (pd.Series or np.ndarray): The single row of real data to explain.\n",
    "        background_data (pd.DataFrame or np.ndarray): Background data used for substitution.\n",
    "        subsets (np.ndarray): Binary subset matrix (1 = real data, 0 = background data).\n",
    "        use_random (bool): \n",
    "            - If False, use mean (continuous) and mode (categorical) for substitution.\n",
    "            - If True, randomly sample background data independently for each subset row.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Substituted data matrix where each row is a generated subset.\n",
    "    \"\"\"\n",
    "    # Convert inputs to NumPy arrays if they are pandas DataFrames or Series\n",
    "    if isinstance(sample_row, pd.Series):\n",
    "        sample_row = sample_row.values\n",
    "    if isinstance(background_data, pd.DataFrame):\n",
    "        background_data = background_data.values\n",
    "\n",
    "    num_features = background_data.shape[1]\n",
    "    num_subsets = subsets.shape[0]\n",
    "\n",
    "    # Step 1: Create a real value matrix by repeating the sample row\n",
    "    real_value_matrix = np.tile(sample_row, (num_subsets, 1))\n",
    "\n",
    "    # Step 2: Create background substitution matrix\n",
    "    if use_random:\n",
    "        # Random sampling: Generate a random sample for each row in subsets\n",
    "        random_indices = np.random.randint(0, background_data.shape[0], size=num_subsets)\n",
    "        background_substitution = background_data[random_indices, :]\n",
    "    else:\n",
    "        # Mean for continuous features and mode for categorical features\n",
    "        background_substitution = []\n",
    "        for i in range(num_features):\n",
    "            unique_vals = np.unique(background_data[:, i])\n",
    "            if len(unique_vals) == 2:  # If feature is categorical\n",
    "                substitution_value = mode(background_data[:, i], keepdims=True).mode[0]\n",
    "            else:  # Continuous\n",
    "                substitution_value = np.mean(background_data[:, i])\n",
    "            background_substitution.append(substitution_value)\n",
    "        background_substitution = np.array(background_substitution)\n",
    "        # Correctly repeat the substitution row for all subsets\n",
    "        background_substitution = np.tile(background_substitution, (num_subsets, 1))\n",
    "\n",
    "    # Step 3: Combine using matrix multiplication\n",
    "    substituted_data = real_value_matrix * subsets + background_substitution * (1 - subsets)\n",
    "\n",
    "    return substituted_data\n",
    "\n",
    "# New Weighted Shapley Calculation Function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Updated Weighted SHAP Function\n",
    "def vectorized_weighted_shap_optimized(model, test_instance, background_data, num_samples=\"auto\", use_random=True):\n",
    "    \"\"\"\n",
    "    Optimized vectorized implementation of Shapley value calculation with adjustments for additivity.\n",
    "\n",
    "    Args:\n",
    "        model (callable): Prediction function of the trained model.\n",
    "        test_instance (np.ndarray): Single test instance to explain.\n",
    "        background_data (np.ndarray): Background data for substitution.\n",
    "        num_samples (int or str): Number of samples. If \"auto\", use default 2 * M + 2048.\n",
    "        use_random (bool): Whether to use random sampling for 0 values.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Adjusted Shapley values for each feature.\n",
    "        float: Baseline prediction (average model prediction on background data).\n",
    "    \"\"\"\n",
    "    num_features = test_instance.shape[0]\n",
    "\n",
    "    # Step 1: Generate subsets and weights\n",
    "    subsets, weights = sample_and_weight(background_data, num_samples=num_samples, weighted_sampling=True)\n",
    "\n",
    "    # Ensure weights sum to 1\n",
    "    weights = weights / np.sum(weights)\n",
    "    assert np.isclose(weights.sum(), 1), \"Weights are not properly normalized!\"\n",
    "\n",
    "    # Step 2: Create the substitution matrix\n",
    "    test_instance_matrix = np.tile(test_instance, (subsets.shape[0], 1))\n",
    "\n",
    "    # Generate background substitution values\n",
    "    if use_random:\n",
    "        random_indices = np.random.randint(0, background_data.shape[0], size=subsets.shape[0])\n",
    "        background_samples = background_data[random_indices, :]\n",
    "    else:\n",
    "        background_mean_mode = np.mean(background_data, axis=0)\n",
    "        background_samples = np.tile(background_mean_mode, (subsets.shape[0], 1))\n",
    "\n",
    "    # Use subsets to replace 1s with test_instance and 0s with background_samples\n",
    "    substituted_matrix = test_instance_matrix * subsets + background_samples * (1 - subsets)\n",
    "\n",
    "    # Step 3: Compute model predictions in batches\n",
    "    predictions = model(substituted_matrix)\n",
    "\n",
    "    # Compute marginal contributions vectorized\n",
    "    contributions = np.zeros((subsets.shape[0], num_features))\n",
    "    for feature_idx in range(num_features):\n",
    "        # Mask for subsets where the feature is excluded\n",
    "        mask_without_feature = subsets[:, feature_idx] == 0\n",
    "\n",
    "        # Create subset matrix where the feature is included\n",
    "        subsets_with_feature = subsets[mask_without_feature].copy()\n",
    "        subsets_with_feature[:, feature_idx] = 1\n",
    "\n",
    "        # Generate the corresponding substitution matrix\n",
    "        substituted_with_feature = test_instance_matrix[mask_without_feature] * subsets_with_feature + \\\n",
    "                                   background_samples[mask_without_feature] * (1 - subsets_with_feature)\n",
    "\n",
    "        # Model predictions for v(S ∪ {X_i})\n",
    "        predictions_with_feature = model(substituted_with_feature)\n",
    "\n",
    "        # Marginal contributions: v(S ∪ {X_i}) - v(S)\n",
    "        contributions[mask_without_feature, feature_idx] = predictions_with_feature - predictions[mask_without_feature]\n",
    "\n",
    "    # Apply weights to contributions and compute weighted average\n",
    "    shapley_values = np.average(contributions, axis=0, weights=weights)\n",
    "\n",
    "    # Step 4: Compute baseline prediction\n",
    "    baseline_prediction = model(background_data).mean()\n",
    "\n",
    "    # Step 5: Enforce Additivity\n",
    "    def enforce_additivity(shapley_values, baseline, prediction):\n",
    "        \"\"\"\n",
    "        Adjust Shapley values to enforce additivity.\n",
    "\n",
    "        Args:\n",
    "            shapley_values (np.ndarray): Calculated Shapley values.\n",
    "            baseline (float): Baseline prediction.\n",
    "            prediction (float): Actual model prediction.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Adjusted Shapley values satisfying additivity.\n",
    "        \"\"\"\n",
    "        total_contributions = np.sum(shapley_values)\n",
    "        residual = prediction - (baseline + total_contributions)\n",
    "\n",
    "        # Redistribute the residual proportionally across features\n",
    "        shapley_values += residual * (shapley_values / total_contributions)\n",
    "\n",
    "        return shapley_values\n",
    "\n",
    "    # Get model prediction for the test instance\n",
    "    prediction = model(test_instance.reshape(1, -1))[0]\n",
    "\n",
    "    # Adjust Shapley values to satisfy additivity\n",
    "    shapley_values = enforce_additivity(shapley_values, baseline_prediction, prediction)\n",
    "\n",
    "    return shapley_values, baseline_prediction\n",
    "\n",
    "\n",
    "\n",
    "# Add a new column to results\n",
    "start_time = time.time()\n",
    "weighted_shap_values, weighted_baseline = vectorized_weighted_shap_optimized(\n",
    "    prediction_function, test_instance, X_train, num_samples=\"auto\"\n",
    ")\n",
    "weighted_time = time.time() - start_time\n",
    "\n",
    "# Add the new weighted SHAP method to the results DataFrame\n",
    "results[\"Weighted SHAP\"] = np.append(np.round(weighted_shap_values, 4), [\n",
    "    np.round(weighted_baseline, 4),\n",
    "    np.round(np.sum(weighted_shap_values) + weighted_baseline, 4),\n",
    "    np.round(weighted_time, 4),\n",
    "    np.round(test_instance_prediction, 4)\n",
    "])\n",
    "\n",
    "# Print the results DataFrame\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc93737-f1cc-4007-b442-6d1bac38fe4b",
   "metadata": {},
   "source": [
    "# based on FastSHAP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fffc0472-4c12-4813-a2b0-52352d5d0be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnoorche\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>TreeExplainer SHAP</th>\n",
       "      <th>KernelExplainer SHAP</th>\n",
       "      <th>MeanKernel SHAP (auto)</th>\n",
       "      <th>Weighted SHAP</th>\n",
       "      <th>NN SHAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feature 1</td>\n",
       "      <td>21.0811</td>\n",
       "      <td>10.7461</td>\n",
       "      <td>24.1879</td>\n",
       "      <td>19.1302</td>\n",
       "      <td>41.3835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feature 2</td>\n",
       "      <td>-7.5035</td>\n",
       "      <td>-8.2388</td>\n",
       "      <td>-15.5479</td>\n",
       "      <td>-8.0885</td>\n",
       "      <td>12.5187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feature 3</td>\n",
       "      <td>1.8309</td>\n",
       "      <td>2.0683</td>\n",
       "      <td>1.9465</td>\n",
       "      <td>1.4556</td>\n",
       "      <td>18.7043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feature 4</td>\n",
       "      <td>-88.9366</td>\n",
       "      <td>-91.7532</td>\n",
       "      <td>-72.6232</td>\n",
       "      <td>-88.2724</td>\n",
       "      <td>-27.4381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feature 5</td>\n",
       "      <td>-33.2918</td>\n",
       "      <td>-34.7344</td>\n",
       "      <td>-42.7720</td>\n",
       "      <td>-31.5424</td>\n",
       "      <td>-35.8953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>8.7755</td>\n",
       "      <td>23.8676</td>\n",
       "      <td>6.7641</td>\n",
       "      <td>9.2730</td>\n",
       "      <td>9.2730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sum</td>\n",
       "      <td>-98.0444</td>\n",
       "      <td>-98.0444</td>\n",
       "      <td>-98.0446</td>\n",
       "      <td>-98.0444</td>\n",
       "      <td>18.5460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Computation Time (s)</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0830</td>\n",
       "      <td>0.0430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prediction (Test Instance)</td>\n",
       "      <td>-98.0444</td>\n",
       "      <td>-98.0444</td>\n",
       "      <td>-98.0444</td>\n",
       "      <td>-98.0444</td>\n",
       "      <td>-98.0444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Feature  TreeExplainer SHAP  KernelExplainer SHAP  \\\n",
       "0                   Feature 1             21.0811               10.7461   \n",
       "1                   Feature 2             -7.5035               -8.2388   \n",
       "2                   Feature 3              1.8309                2.0683   \n",
       "3                   Feature 4            -88.9366              -91.7532   \n",
       "4                   Feature 5            -33.2918              -34.7344   \n",
       "0                    Baseline              8.7755               23.8676   \n",
       "1                         Sum            -98.0444              -98.0444   \n",
       "2        Computation Time (s)              0.0350                0.0450   \n",
       "3  Prediction (Test Instance)            -98.0444              -98.0444   \n",
       "\n",
       "   MeanKernel SHAP (auto)  Weighted SHAP  NN SHAP  \n",
       "0                 24.1879        19.1302  41.3835  \n",
       "1                -15.5479        -8.0885  12.5187  \n",
       "2                  1.9465         1.4556  18.7043  \n",
       "3                -72.6232       -88.2724 -27.4381  \n",
       "4                -42.7720       -31.5424 -35.8953  \n",
       "0                  6.7641         9.2730   9.2730  \n",
       "1                -98.0446       -98.0444  18.5460  \n",
       "2                  0.0140         0.0830   0.0430  \n",
       "3                -98.0444       -98.0444 -98.0444  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def additivity_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Loss function with balanced regression loss and additivity constraint.\n",
    "\n",
    "    Args:\n",
    "        y_true: True SHAP values.\n",
    "        y_pred: Predicted SHAP values.\n",
    "\n",
    "    Returns:\n",
    "        Regression loss with dynamically scaled additivity constraint.\n",
    "    \"\"\"\n",
    "    shap_sum = tf.reduce_sum(y_pred, axis=1, keepdims=True)\n",
    "    baseline = tf.reduce_mean(y_true, axis=1, keepdims=True)\n",
    "\n",
    "    # Regression loss\n",
    "    regression_loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "    # Additivity constraint with reduced weight\n",
    "    additivity_constraint = tf.reduce_mean(tf.square(shap_sum - baseline))\n",
    "    return regression_loss + 0.1 * additivity_constraint  # Reduced weight\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def select_representative_subset(X_train, n_clusters=10, subset_size=800, use_clustering=True):\n",
    "    \"\"\"\n",
    "    Select a representative subset of the training data.\n",
    "    \n",
    "    If `use_clustering` is True, the subset is selected using clustering (KMeans). \n",
    "    Otherwise, random sampling without replacement is performed.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Training data.\n",
    "        n_clusters (int): Number of clusters for KMeans (used only if `use_clustering=True`).\n",
    "        subset_size (int): Number of samples to select for the subset.\n",
    "        use_clustering (bool): Whether to use clustering for subset selection.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Subset of training data.\n",
    "    \"\"\"\n",
    "    if not use_clustering:\n",
    "        # Random sampling without replacement\n",
    "        subset_indices = np.random.choice(X_train.shape[0], size=subset_size, replace=False)\n",
    "        return X_train[subset_indices]\n",
    "\n",
    "    # Clustering-based subset selection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(X_train)\n",
    "\n",
    "    cluster_counts = np.bincount(cluster_labels)\n",
    "    total_points = X_train.shape[0]\n",
    "\n",
    "    samples_per_cluster = np.round((cluster_counts / total_points) * subset_size).astype(int)\n",
    "\n",
    "    diff = subset_size - np.sum(samples_per_cluster)\n",
    "    while diff != 0:\n",
    "        for cluster_id in range(n_clusters):\n",
    "            if diff == 0:\n",
    "                break\n",
    "            if diff > 0 and samples_per_cluster[cluster_id] < cluster_counts[cluster_id]:\n",
    "                samples_per_cluster[cluster_id] += 1\n",
    "                diff -= 1\n",
    "            elif diff < 0 and samples_per_cluster[cluster_id] > 1:\n",
    "                samples_per_cluster[cluster_id] -= 1\n",
    "                diff += 1\n",
    "\n",
    "    subset_indices = []\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "        if len(cluster_indices) <= samples_per_cluster[cluster_id]:\n",
    "            subset_indices.extend(cluster_indices)\n",
    "        else:\n",
    "            subset_indices.extend(\n",
    "                np.random.choice(cluster_indices, size=samples_per_cluster[cluster_id], replace=False)\n",
    "            )\n",
    "\n",
    "    subset_indices = np.array(subset_indices)\n",
    "    return X_train[subset_indices]\n",
    "\n",
    "\n",
    "def train_nn_on_weighted_shap(\n",
    "    model, X_train, save_path=\"shap_nn_model.keras\", n_clusters=10, subset_size=800, num_samples=\"auto\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a neural network using Weighted SHAP values from representative subsets.\n",
    "    \"\"\"\n",
    "    X_subset = select_representative_subset(X_train, n_clusters, subset_size)\n",
    "\n",
    "    shap_values_list = []\n",
    "    baseline = np.mean(model(X_train))  # Average prediction for all background data\n",
    "\n",
    "    for i in range(X_subset.shape[0]):\n",
    "        shap_values, _ = vectorized_weighted_shap_optimized(\n",
    "            model, X_subset[i], X_train, num_samples=num_samples, use_random=False\n",
    "        )\n",
    "        shap_values_list.append(shap_values)\n",
    "\n",
    "    shap_values_array = np.array(shap_values_list)\n",
    "\n",
    "    # Apply feature-wise scaling to SHAP values\n",
    "    shap_scaler = StandardScaler()\n",
    "    shap_values_normalized = shap_scaler.fit_transform(shap_values_array)\n",
    "\n",
    "    # Create DataFrame for SHAP values and input features\n",
    "    X_shap_df = pd.DataFrame(X_subset, columns=[f\"Feature {i+1}\" for i in range(X_subset.shape[1])])\n",
    "    for i in range(shap_values_array.shape[1]):\n",
    "        X_shap_df[f\"SHAP Feature {i+1}\"] = shap_values_array[:, i]\n",
    "\n",
    "    # Train Neural Network\n",
    "    num_features = X_train.shape[1]\n",
    "    X_nn = X_shap_df[[f\"Feature {i+1}\" for i in range(num_features)]].values\n",
    "    X_train_nn, X_val_nn, y_train_nn, y_val_nn = train_test_split(X_nn, shap_values_normalized, test_size=0.2, random_state=42)\n",
    "\n",
    "    nn_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(num_features,)),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\", kernel_initializer=\"he_normal\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\", kernel_initializer=\"he_normal\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Dense(num_features, activation=\"linear\")\n",
    "    ])\n",
    "\n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    nn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=additivity_loss, metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "    nn_model.fit(X_train_nn, y_train_nn, validation_data=(X_val_nn, y_val_nn), epochs=300, batch_size=64, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    nn_model.save(save_path)\n",
    "\n",
    "    return baseline, shap_scaler\n",
    "\n",
    "\n",
    "def predict_nn_shap(model_path, test_instance, baseline, shap_scaler):\n",
    "    \"\"\"\n",
    "    Predict SHAP values for a test instance using a saved neural network model.\n",
    "    \"\"\"\n",
    "    nn_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    start_time = time.time()\n",
    "    nn_shap_values_scaled = nn_model.predict(test_instance.reshape(1, -1)).flatten()\n",
    "    nn_shap_values = shap_scaler.inverse_transform(nn_shap_values_scaled.reshape(1, -1)).flatten()\n",
    "    prediction_time = time.time() - start_time\n",
    "\n",
    "    # Adjust SHAP values to enforce additivity\n",
    "    shap_sum = np.sum(nn_shap_values)\n",
    "    adjustment = baseline - shap_sum\n",
    "    nn_shap_values += adjustment / len(nn_shap_values)\n",
    "\n",
    "    num_features = test_instance.shape[0]\n",
    "    results = pd.DataFrame(\n",
    "        {\n",
    "            \"Feature\": [f\"Feature {i+1}\" for i in range(num_features)],\n",
    "            \"NN SHAP\": nn_shap_values,\n",
    "        }\n",
    "    )\n",
    "    results.loc[len(results.index)] = {\"Feature\": \"Baseline\", \"NN SHAP\": baseline}\n",
    "    results.loc[len(results.index)] = {\"Feature\": \"Prediction Time (s)\", \"NN SHAP\": prediction_time}\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example Workflow\n",
    "baseline, shap_scaler = train_nn_on_weighted_shap(\n",
    "    model=prediction_function,\n",
    "    X_train=X_train,\n",
    "    save_path=\"shap_nn_model.keras\",\n",
    "    n_clusters=1,\n",
    "    subset_size=1000,\n",
    "    num_samples=\"auto\"\n",
    ")\n",
    "\n",
    "nn_results = predict_nn_shap(\n",
    "    model_path=\"shap_nn_model.keras\",\n",
    "    test_instance=test_instance,\n",
    "    baseline=baseline,\n",
    "    shap_scaler=shap_scaler\n",
    ")\n",
    "\n",
    "# Update Results DataFrame\n",
    "nn_shap_values = nn_results.loc[nn_results[\"Feature\"].str.contains(\"Feature\"), \"NN SHAP\"].values\n",
    "nn_baseline = nn_results.loc[nn_results[\"Feature\"] == \"Baseline\", \"NN SHAP\"].values[0]\n",
    "results[\"NN SHAP\"] = np.append(np.round(nn_shap_values, 4), [\n",
    "    np.round(nn_baseline, 4),\n",
    "    np.round(np.sum(nn_shap_values) + nn_baseline, 4),\n",
    "    np.round(nn_results.loc[nn_results[\"Feature\"] == \"Prediction Time (s)\", \"NN SHAP\"].values[0], 4),\n",
    "    np.round(test_instance_prediction, 4)\n",
    "])\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
