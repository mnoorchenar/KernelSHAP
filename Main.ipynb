{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf480ca-4c1d-4eb9-87d9-3f571ed800ed",
   "metadata": {},
   "source": [
    "# Basic Kernel\n",
    "https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/model_agnostic/Simple%20Kernel%20SHAP.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9009932f-e1d0-4b45-8c87-a1756178bbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnoorche\\AppData\\Local\\Temp\\ipykernel_21016\\4159486803.py:72: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  tree_baseline = float(tree_explainer.expected_value)  # Ensure scalar\n",
      "Using 800 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967c75e3898c4c94b24c92351be6b5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>TreeExplainer SHAP</th>\n",
       "      <th>KernelExplainer SHAP</th>\n",
       "      <th>CustomKernel SHAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feature 1</td>\n",
       "      <td>13.4197</td>\n",
       "      <td>13.4311</td>\n",
       "      <td>12.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feature 2</td>\n",
       "      <td>1.7568</td>\n",
       "      <td>1.3560</td>\n",
       "      <td>3.4165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feature 3</td>\n",
       "      <td>1.9600</td>\n",
       "      <td>0.9772</td>\n",
       "      <td>-0.5405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feature 4</td>\n",
       "      <td>75.3594</td>\n",
       "      <td>74.8056</td>\n",
       "      <td>72.5709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feature 5</td>\n",
       "      <td>1.0264</td>\n",
       "      <td>0.5648</td>\n",
       "      <td>-0.5133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Feature 6</td>\n",
       "      <td>-3.2434</td>\n",
       "      <td>-3.4140</td>\n",
       "      <td>-3.5330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Feature 7</td>\n",
       "      <td>-30.7180</td>\n",
       "      <td>-31.2188</td>\n",
       "      <td>-33.8595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Feature 8</td>\n",
       "      <td>1.8544</td>\n",
       "      <td>1.8552</td>\n",
       "      <td>1.8361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Feature 9</td>\n",
       "      <td>1.4149</td>\n",
       "      <td>0.8683</td>\n",
       "      <td>0.1188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Feature 10</td>\n",
       "      <td>-71.4951</td>\n",
       "      <td>-68.8819</td>\n",
       "      <td>-69.9190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>-1.5242</td>\n",
       "      <td>-0.5323</td>\n",
       "      <td>7.8329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum</th>\n",
       "      <td>Sum</td>\n",
       "      <td>-10.1890</td>\n",
       "      <td>-10.1890</td>\n",
       "      <td>-10.1883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computation Time (s)</th>\n",
       "      <td>Computation Time (s)</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>12.2340</td>\n",
       "      <td>0.0350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Feature  TreeExplainer SHAP  \\\n",
       "0                                Feature 1             13.4197   \n",
       "1                                Feature 2              1.7568   \n",
       "2                                Feature 3              1.9600   \n",
       "3                                Feature 4             75.3594   \n",
       "4                                Feature 5              1.0264   \n",
       "5                                Feature 6             -3.2434   \n",
       "6                                Feature 7            -30.7180   \n",
       "7                                Feature 8              1.8544   \n",
       "8                                Feature 9              1.4149   \n",
       "9                               Feature 10            -71.4951   \n",
       "Baseline                          Baseline             -1.5242   \n",
       "Sum                                    Sum            -10.1890   \n",
       "Computation Time (s)  Computation Time (s)              0.0230   \n",
       "\n",
       "                      KernelExplainer SHAP  CustomKernel SHAP  \n",
       "0                                  13.4311            12.4019  \n",
       "1                                   1.3560             3.4165  \n",
       "2                                   0.9772            -0.5405  \n",
       "3                                  74.8056            72.5709  \n",
       "4                                   0.5648            -0.5133  \n",
       "5                                  -3.4140            -3.5330  \n",
       "6                                 -31.2188           -33.8595  \n",
       "7                                   1.8552             1.8361  \n",
       "8                                   0.8683             0.1188  \n",
       "9                                 -68.8819           -69.9190  \n",
       "Baseline                           -0.5323             7.8329  \n",
       "Sum                               -10.1890           -10.1883  \n",
       "Computation Time (s)               12.2340             0.0350  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "import scipy.special\n",
    "import time\n",
    "import shap\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Generate a simple regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Train a RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Select the first test instance for explanation\n",
    "test_instance = X_test[0]\n",
    "\n",
    "# Define helper functions for Kernel SHAP\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(len(s) + 1))\n",
    "\n",
    "def shapley_kernel(M, s):\n",
    "    if s == 0 or s == M:\n",
    "        return 10000  # Large constant for numerical stability\n",
    "    return (M - 1) / (scipy.special.binom(M, s) * s * (M - s))\n",
    "\n",
    "def kernel_shap(f, x, reference, M):\n",
    "    X = np.zeros((2**M, M + 1))\n",
    "    X[:, -1] = 1\n",
    "    weights = np.zeros(2**M)\n",
    "    V = np.zeros((2**M, M))\n",
    "    for i in range(2**M):\n",
    "        V[i, :] = reference\n",
    "\n",
    "    for i, s in enumerate(powerset(range(M))):\n",
    "        s = list(s)\n",
    "        V[i, s] = x[s]\n",
    "        X[i, s] = 1\n",
    "        weights[i] = shapley_kernel(M, len(s))\n",
    "    y = f(V)\n",
    "    wsq = np.sqrt(weights)\n",
    "    result = np.linalg.lstsq(wsq[:, None] * X, wsq * y, rcond=None)[0]\n",
    "    return result\n",
    "\n",
    "# Define the prediction function\n",
    "def prediction_function(X):\n",
    "    return regressor.predict(X)\n",
    "\n",
    "# Define the reference input (mean of the training data)\n",
    "reference = np.mean(X_train, axis=0)\n",
    "\n",
    "# Number of features\n",
    "M = X_train.shape[1]\n",
    "\n",
    "# Step 6: Compute SHAP values and time the computation\n",
    "\n",
    "# TreeExplainer SHAP\n",
    "start_time = time.time()\n",
    "tree_explainer = shap.TreeExplainer(regressor)\n",
    "tree_shap_values = tree_explainer.shap_values(test_instance.reshape(1, -1))[0]\n",
    "tree_baseline = float(tree_explainer.expected_value)  # Ensure scalar\n",
    "tree_time = time.time() - start_time\n",
    "\n",
    "# KernelExplainer SHAP\n",
    "start_time = time.time()\n",
    "kernel_explainer = shap.KernelExplainer(prediction_function, X_train)\n",
    "kernel_shap_values = kernel_explainer.shap_values(test_instance.reshape(1, -1))[0]\n",
    "kernel_baseline = float(kernel_explainer.expected_value)  # Ensure scalar\n",
    "kernel_time = time.time() - start_time\n",
    "\n",
    "# Custom Kernel SHAP\n",
    "start_time = time.time()\n",
    "custom_kernel_phi = kernel_shap(prediction_function, test_instance, reference, M)\n",
    "custom_baseline = float(custom_kernel_phi[-1])  # Baseline (intercept), ensure scalar\n",
    "custom_kernel_shap_values = custom_kernel_phi[:-1]  # SHAP values for features\n",
    "custom_kernel_time = time.time() - start_time  # Time taken for Custom Kernel SHAP\n",
    "\n",
    "# Step 7: Display Results\n",
    "results = pd.DataFrame({\n",
    "    \"Feature\": [f\"Feature {i+1}\" for i in range(X.shape[1])],\n",
    "    \"TreeExplainer SHAP\": np.round(tree_shap_values, 4),  # TreeExplainer output\n",
    "    \"KernelExplainer SHAP\": np.round(kernel_shap_values, 4),  # KernelExplainer output\n",
    "    \"CustomKernel SHAP\": np.round(custom_kernel_shap_values, 4),  # Custom Kernel SHAP\n",
    "})\n",
    "\n",
    "# Add baselines to the results\n",
    "results.loc[\"Baseline\"] = {\n",
    "    \"Feature\": \"Baseline\",\n",
    "    \"TreeExplainer SHAP\": np.round(tree_baseline, 4),\n",
    "    \"KernelExplainer SHAP\": np.round(kernel_baseline, 4),\n",
    "    \"CustomKernel SHAP\": np.round(custom_baseline, 4),\n",
    "}\n",
    "\n",
    "# Calculate the sum of SHAP values + baseline for each method\n",
    "tree_shap_sum = float(np.sum(tree_shap_values) + tree_baseline)  # Ensure scalar\n",
    "kernel_shap_sum = float(np.sum(kernel_shap_values) + kernel_baseline)  # Ensure scalar\n",
    "custom_shap_sum = float(np.sum(custom_kernel_shap_values) + custom_baseline)  # Ensure scalar\n",
    "\n",
    "results.loc[\"Sum\"] = {\n",
    "    \"Feature\": \"Sum\",\n",
    "    \"TreeExplainer SHAP\": np.round(tree_shap_sum, 4),\n",
    "    \"KernelExplainer SHAP\": np.round(kernel_shap_sum, 4),\n",
    "    \"CustomKernel SHAP\": np.round(custom_shap_sum, 4),\n",
    "}\n",
    "\n",
    "# Add computation times as the last row\n",
    "results.loc[\"Computation Time (s)\"] = {\n",
    "    \"Feature\": \"Computation Time (s)\",\n",
    "    \"TreeExplainer SHAP\": np.round(tree_time, 4),\n",
    "    \"KernelExplainer SHAP\": np.round(kernel_time, 4),\n",
    "    \"CustomKernel SHAP\": np.round(custom_kernel_time, 4),\n",
    "}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd34993-d749-42ba-8e2e-6a4227c81272",
   "metadata": {},
   "source": [
    "# Updated Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ac7484cc-971f-4e3e-b6b1-f203aa88b6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnoorche\\AppData\\Local\\Temp\\ipykernel_21016\\1762167479.py:117: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  tree_baseline = float(tree_explainer.expected_value)  # Ensure scalar\n",
      "Using 500 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9775775e51764a468fb025260b8a90e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>TreeExplainer SHAP</th>\n",
       "      <th>KernelExplainer SHAP</th>\n",
       "      <th>CustomKernel SHAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feature 1</td>\n",
       "      <td>-47.8522</td>\n",
       "      <td>-50.7102</td>\n",
       "      <td>-49.7264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feature 2</td>\n",
       "      <td>9.0346</td>\n",
       "      <td>5.7863</td>\n",
       "      <td>9.8388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feature 3</td>\n",
       "      <td>3.3038</td>\n",
       "      <td>-2.5937</td>\n",
       "      <td>0.9901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feature 4</td>\n",
       "      <td>-2.1609</td>\n",
       "      <td>-4.1729</td>\n",
       "      <td>-17.3094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feature 5</td>\n",
       "      <td>-0.7324</td>\n",
       "      <td>1.7503</td>\n",
       "      <td>-0.6865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Feature 6</td>\n",
       "      <td>-1.6299</td>\n",
       "      <td>-1.6725</td>\n",
       "      <td>4.1388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Feature 7</td>\n",
       "      <td>25.5404</td>\n",
       "      <td>14.7520</td>\n",
       "      <td>4.7303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Feature 8</td>\n",
       "      <td>35.0099</td>\n",
       "      <td>29.4719</td>\n",
       "      <td>30.4919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Feature 9</td>\n",
       "      <td>-74.1182</td>\n",
       "      <td>-83.5356</td>\n",
       "      <td>-88.8265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Feature 10</td>\n",
       "      <td>-115.6296</td>\n",
       "      <td>-82.9742</td>\n",
       "      <td>-79.2923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>1.3357</td>\n",
       "      <td>5.9997</td>\n",
       "      <td>22.7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sum</td>\n",
       "      <td>-167.8988</td>\n",
       "      <td>-167.8988</td>\n",
       "      <td>-162.8872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Computation Time (s)</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>6.5550</td>\n",
       "      <td>0.0130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Feature  TreeExplainer SHAP  KernelExplainer SHAP  \\\n",
       "0             Feature 1            -47.8522              -50.7102   \n",
       "1             Feature 2              9.0346                5.7863   \n",
       "2             Feature 3              3.3038               -2.5937   \n",
       "3             Feature 4             -2.1609               -4.1729   \n",
       "4             Feature 5             -0.7324                1.7503   \n",
       "5             Feature 6             -1.6299               -1.6725   \n",
       "6             Feature 7             25.5404               14.7520   \n",
       "7             Feature 8             35.0099               29.4719   \n",
       "8             Feature 9            -74.1182              -83.5356   \n",
       "9            Feature 10           -115.6296              -82.9742   \n",
       "0              Baseline              1.3357                5.9997   \n",
       "1                   Sum           -167.8988             -167.8988   \n",
       "2  Computation Time (s)              0.0260                6.5550   \n",
       "\n",
       "   CustomKernel SHAP  \n",
       "0           -49.7264  \n",
       "1             9.8388  \n",
       "2             0.9901  \n",
       "3           -17.3094  \n",
       "4            -0.6865  \n",
       "5             4.1388  \n",
       "6             4.7303  \n",
       "7            30.4919  \n",
       "8           -88.8265  \n",
       "9           -79.2923  \n",
       "0            22.7639  \n",
       "1          -162.8872  \n",
       "2             0.0130  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "import scipy.special\n",
    "import time\n",
    "import shap\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Generate a simple regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Train a RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Select the first test instance for explanation\n",
    "test_instance = X_test[0]\n",
    "\n",
    "# Monte Carlo-based sampling for subsets\n",
    "def sample_subsets(M, num_samples):\n",
    "    \"\"\"\n",
    "    Generate random subsets of features using Monte Carlo sampling.\n",
    "    This replaces the exhaustive powerset generation for efficiency.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    subsets = []\n",
    "    for _ in range(num_samples):\n",
    "        subset = np.random.choice([0, 1], size=M, p=[0.5, 0.5])\n",
    "        subsets.append(np.where(subset == 1)[0])  # Indices of features in the subset\n",
    "    return subsets\n",
    "\n",
    "# Shapley kernel weight calculation\n",
    "def shapley_kernel(M, s):\n",
    "    \"\"\"\n",
    "    Compute the Shapley kernel weight for a given subset size `s`.\n",
    "    \"\"\"\n",
    "    if s == 0 or s == M:\n",
    "        return 10000  # Large constant for numerical stability\n",
    "    return (M - 1) / (scipy.special.binom(M, s) * s * (M - s))\n",
    "\n",
    "# Kernel SHAP implementation with optional Monte Carlo sampling\n",
    "def kernel_shap(f, x, reference, M, num_samples=None):\n",
    "    \"\"\"\n",
    "    Compute SHAP values using the Kernel SHAP method.\n",
    "    - If `num_samples` is None, compute exact SHAP values (2^M subsets).\n",
    "    - Otherwise, approximate SHAP values using Monte Carlo sampling.\n",
    "\n",
    "    Args:\n",
    "        f (callable): Prediction function.\n",
    "        x (np.ndarray): Input instance to explain.\n",
    "        reference (np.ndarray): Reference values for each feature.\n",
    "        M (int): Number of features.\n",
    "        num_samples (int): Number of Monte Carlo samples (optional).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: SHAP values for the input instance.\n",
    "    \"\"\"\n",
    "    if num_samples is None:\n",
    "        # Use exact computation (exhaustive powerset)\n",
    "        subsets = list(itertools.chain.from_iterable(itertools.combinations(range(M), r) for r in range(M + 1)))\n",
    "    else:\n",
    "        # Use Monte Carlo sampling\n",
    "        subsets = sample_subsets(M, num_samples)\n",
    "\n",
    "    # Prepare matrices for least-squares regression\n",
    "    num_subsets = len(subsets)\n",
    "    X = np.zeros((num_subsets, M + 1))\n",
    "    X[:, -1] = 1  # Bias term\n",
    "    V = np.tile(reference, (num_subsets, 1))\n",
    "    weights = np.zeros(num_subsets)\n",
    "\n",
    "    # Populate X, V, and weights based on subsets\n",
    "    for i, s in enumerate(subsets):\n",
    "        s = list(s)\n",
    "        V[i, s] = x[s]\n",
    "        X[i, s] = 1\n",
    "        weights[i] = shapley_kernel(M, len(s))\n",
    "\n",
    "    # Normalize weights for numerical stability\n",
    "    weights /= np.sum(weights)\n",
    "\n",
    "    # Model predictions for perturbed inputs\n",
    "    y = f(V)\n",
    "\n",
    "    # Weighted least-squares regression\n",
    "    wsq = np.sqrt(weights)\n",
    "    result = np.linalg.lstsq(wsq[:, None] * X, wsq * y, rcond=None)[0]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Define the prediction function\n",
    "def prediction_function(X):\n",
    "    return regressor.predict(X)\n",
    "\n",
    "# Define the reference input (mean of the training data)\n",
    "reference = np.mean(X_train, axis=0)\n",
    "\n",
    "# Number of features\n",
    "M = X_train.shape[1]\n",
    "\n",
    "# Step 6: Compute SHAP values and time the computation\n",
    "\n",
    "# TreeExplainer SHAP\n",
    "start_time = time.time()\n",
    "tree_explainer = shap.TreeExplainer(regressor)\n",
    "tree_shap_values = tree_explainer.shap_values(test_instance.reshape(1, -1))[0]\n",
    "tree_baseline = float(tree_explainer.expected_value)  # Ensure scalar\n",
    "tree_time = time.time() - start_time\n",
    "\n",
    "# KernelExplainer SHAP\n",
    "start_time = time.time()\n",
    "kernel_explainer = shap.KernelExplainer(prediction_function, np.tile(reference, (500, 1)))\n",
    "kernel_shap_values = kernel_explainer.shap_values(test_instance.reshape(1, -1))[0]\n",
    "kernel_baseline = float(kernel_explainer.expected_value)  # Ensure scalar\n",
    "kernel_time = time.time() - start_time\n",
    "\n",
    "# Custom Kernel SHAP\n",
    "start_time = time.time()\n",
    "num_samples = 100  # Increase Monte Carlo samples for accuracy\n",
    "custom_kernel_phi = kernel_shap(prediction_function, test_instance, reference, M, num_samples)\n",
    "custom_baseline = float(custom_kernel_phi[-1])  # Baseline (intercept), ensure scalar\n",
    "custom_kernel_shap_values = custom_kernel_phi[:-1]  # SHAP values for features\n",
    "custom_kernel_time = time.time() - start_time  # Time taken for Custom Kernel SHAP\n",
    "\n",
    "# Step 7: Display Results\n",
    "results = pd.DataFrame({\n",
    "    \"Feature\": [f\"Feature {i+1}\" for i in range(X.shape[1])],\n",
    "    \"TreeExplainer SHAP\": np.round(tree_shap_values, 4),  # TreeExplainer output\n",
    "    \"KernelExplainer SHAP\": np.round(kernel_shap_values, 4),  # KernelExplainer output\n",
    "    \"CustomKernel SHAP\": np.round(custom_kernel_shap_values, 4),  # Custom Kernel SHAP\n",
    "})\n",
    "\n",
    "# Add baselines to the results\n",
    "results = pd.concat([\n",
    "    results,\n",
    "    pd.DataFrame([\n",
    "        {\"Feature\": \"Baseline\", \"TreeExplainer SHAP\": np.round(tree_baseline, 4),\n",
    "         \"KernelExplainer SHAP\": np.round(kernel_baseline, 4),\n",
    "         \"CustomKernel SHAP\": np.round(custom_baseline, 4)},\n",
    "        {\"Feature\": \"Sum\", \"TreeExplainer SHAP\": np.round(np.sum(tree_shap_values) + tree_baseline, 4),\n",
    "         \"KernelExplainer SHAP\": np.round(np.sum(kernel_shap_values) + kernel_baseline, 4),\n",
    "         \"CustomKernel SHAP\": np.round(np.sum(custom_kernel_shap_values) + custom_baseline, 4)},\n",
    "        {\"Feature\": \"Computation Time (s)\", \"TreeExplainer SHAP\": np.round(tree_time, 4),\n",
    "         \"KernelExplainer SHAP\": np.round(kernel_time, 4),\n",
    "         \"CustomKernel SHAP\": np.round(custom_kernel_time, 4)},\n",
    "    ])\n",
    "])\n",
    "\n",
    "# Print the results\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c889ee-1228-48d1-854f-698d021fc434",
   "metadata": {},
   "source": [
    "# cosine with reference point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f152d52e-7125-40b8-ba7d-d9233527595e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnoorche\\AppData\\Local\\Temp\\ipykernel_21016\\1400622272.py:158: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  tree_baseline = float(tree_explainer.expected_value)  # Ensure scalar\n",
      "Using 800 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e20ace314934f908e3c524a76b1beea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>TreeExplainer SHAP</th>\n",
       "      <th>KernelExplainer SHAP</th>\n",
       "      <th>MeanKernel SHAP</th>\n",
       "      <th>FarKernel SHAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feature 1</td>\n",
       "      <td>13.4197</td>\n",
       "      <td>13.4311</td>\n",
       "      <td>12.4019</td>\n",
       "      <td>5.1130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feature 2</td>\n",
       "      <td>1.7568</td>\n",
       "      <td>1.3560</td>\n",
       "      <td>3.4165</td>\n",
       "      <td>17.8503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feature 3</td>\n",
       "      <td>1.9600</td>\n",
       "      <td>0.9772</td>\n",
       "      <td>-0.5405</td>\n",
       "      <td>35.1193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feature 4</td>\n",
       "      <td>75.3594</td>\n",
       "      <td>74.8056</td>\n",
       "      <td>72.5709</td>\n",
       "      <td>103.4481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feature 5</td>\n",
       "      <td>1.0264</td>\n",
       "      <td>0.5648</td>\n",
       "      <td>-0.5133</td>\n",
       "      <td>25.5756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Feature 6</td>\n",
       "      <td>-3.2434</td>\n",
       "      <td>-3.4140</td>\n",
       "      <td>-3.5330</td>\n",
       "      <td>-21.5159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Feature 7</td>\n",
       "      <td>-30.7180</td>\n",
       "      <td>-31.2188</td>\n",
       "      <td>-33.8595</td>\n",
       "      <td>-61.2246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Feature 8</td>\n",
       "      <td>1.8544</td>\n",
       "      <td>1.8552</td>\n",
       "      <td>1.8361</td>\n",
       "      <td>12.0021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Feature 9</td>\n",
       "      <td>1.4149</td>\n",
       "      <td>0.8683</td>\n",
       "      <td>0.1188</td>\n",
       "      <td>-23.7780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Feature 10</td>\n",
       "      <td>-71.4951</td>\n",
       "      <td>-68.8819</td>\n",
       "      <td>-69.9190</td>\n",
       "      <td>-97.2885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>-1.5242</td>\n",
       "      <td>-0.5323</td>\n",
       "      <td>7.8329</td>\n",
       "      <td>-5.4910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sum</td>\n",
       "      <td>-10.1890</td>\n",
       "      <td>-10.1890</td>\n",
       "      <td>-10.1883</td>\n",
       "      <td>-10.1896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Computation Time (s)</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>12.2440</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>0.0580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Feature  TreeExplainer SHAP  KernelExplainer SHAP  \\\n",
       "0             Feature 1             13.4197               13.4311   \n",
       "1             Feature 2              1.7568                1.3560   \n",
       "2             Feature 3              1.9600                0.9772   \n",
       "3             Feature 4             75.3594               74.8056   \n",
       "4             Feature 5              1.0264                0.5648   \n",
       "5             Feature 6             -3.2434               -3.4140   \n",
       "6             Feature 7            -30.7180              -31.2188   \n",
       "7             Feature 8              1.8544                1.8552   \n",
       "8             Feature 9              1.4149                0.8683   \n",
       "9            Feature 10            -71.4951              -68.8819   \n",
       "0              Baseline             -1.5242               -0.5323   \n",
       "1                   Sum            -10.1890              -10.1890   \n",
       "2  Computation Time (s)              0.0230               12.2440   \n",
       "\n",
       "   MeanKernel SHAP  FarKernel SHAP  \n",
       "0          12.4019          5.1130  \n",
       "1           3.4165         17.8503  \n",
       "2          -0.5405         35.1193  \n",
       "3          72.5709        103.4481  \n",
       "4          -0.5133         25.5756  \n",
       "5          -3.5330        -21.5159  \n",
       "6         -33.8595        -61.2246  \n",
       "7           1.8361         12.0021  \n",
       "8           0.1188        -23.7780  \n",
       "9         -69.9190        -97.2885  \n",
       "0           7.8329         -5.4910  \n",
       "1         -10.1883        -10.1896  \n",
       "2           0.0360          0.0580  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "import scipy.special\n",
    "import time\n",
    "import shap\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Generate a simple regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Train a RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Select the first test instance for explanation\n",
    "test_instance = X_test[0]\n",
    "\n",
    "# Shapley kernel weight calculation\n",
    "def shapley_kernel(M, s):\n",
    "    \"\"\"\n",
    "    Compute the Shapley kernel weight for a given subset size `s`.\n",
    "    \"\"\"\n",
    "    if s == 0 or s == M:\n",
    "        return 10000  # Large constant for numerical stability\n",
    "    return (M - 1) / (scipy.special.binom(M, s) * s * (M - s))\n",
    "\n",
    "# Mean Kernel SHAP function\n",
    "def mean_kernel_shap(f, x, reference, M):\n",
    "    \"\"\"\n",
    "    Compute SHAP values by substituting excluded features with the mean of the features.\n",
    "\n",
    "    Args:\n",
    "        f (callable): Prediction function.\n",
    "        x (np.ndarray): Input instance to explain.\n",
    "        reference (np.ndarray): Mean feature values (reference).\n",
    "        M (int): Number of features.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: SHAP values for the input instance.\n",
    "    \"\"\"\n",
    "    # Generate all subsets (powerset of features)\n",
    "    subsets = list(itertools.chain.from_iterable(itertools.combinations(range(M), r) for r in range(M + 1)))\n",
    "    num_subsets = len(subsets)\n",
    "    \n",
    "    # Prepare matrices for least-squares regression\n",
    "    X = np.zeros((num_subsets, M + 1))\n",
    "    X[:, -1] = 1  # Bias term\n",
    "    V = np.zeros((num_subsets, M))\n",
    "\n",
    "    # Populate X, V, and weights\n",
    "    weights = np.zeros(num_subsets)\n",
    "    for i, s in enumerate(subsets):\n",
    "        s = list(s)\n",
    "        V[i, :] = reference  # Start with the mean (reference)\n",
    "        V[i, s] = x[s]       # Replace included features with values from `x`\n",
    "        X[i, s] = 1\n",
    "        weights[i] = shapley_kernel(M, len(s))\n",
    "\n",
    "    # Normalize weights for numerical stability\n",
    "    weights /= np.sum(weights)\n",
    "\n",
    "    # Model predictions for perturbed inputs\n",
    "    y = f(V)\n",
    "\n",
    "    # Weighted least-squares regression\n",
    "    wsq = np.sqrt(weights)\n",
    "    result = np.linalg.lstsq(wsq[:, None] * X, wsq * y, rcond=None)[0]\n",
    "\n",
    "    return result\n",
    "\n",
    "# Kernel SHAP with farthest points\n",
    "def kernel_shap_with_far_points(f, x, X_train, M, num_samples=None, k=10):\n",
    "    \"\"\"\n",
    "    Compute SHAP values using a modified Kernel SHAP method.\n",
    "    - Replace excluded features with random samples from the farthest `k` training points.\n",
    "\n",
    "    Args:\n",
    "        f (callable): Prediction function.\n",
    "        x (np.ndarray): Input instance to explain.\n",
    "        X_train (np.ndarray): Training dataset (used to find far points).\n",
    "        M (int): Number of features.\n",
    "        num_samples (int): Number of Monte Carlo samples (optional).\n",
    "        k (int): Number of farthest points to sample for replacing excluded features.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: SHAP values for the input instance.\n",
    "    \"\"\"\n",
    "    # Compute distances from the test instance to all training instances\n",
    "    distances = np.linalg.norm(X_train - x, axis=1)\n",
    "    \n",
    "    # Find the farthest `k` points\n",
    "    farthest_indices = np.argsort(distances)[-k:]  # Indices of the k farthest points\n",
    "    farthest_points = X_train[farthest_indices]\n",
    "    \n",
    "    # Monte Carlo sampling or exhaustive computation\n",
    "    if num_samples is None:\n",
    "        subsets = list(itertools.chain.from_iterable(itertools.combinations(range(M), r) for r in range(M + 1)))\n",
    "    else:\n",
    "        subsets = sample_subsets(M, num_samples)\n",
    "\n",
    "    # Prepare matrices for least-squares regression\n",
    "    num_subsets = len(subsets)\n",
    "    X = np.zeros((num_subsets, M + 1))\n",
    "    X[:, -1] = 1  # Bias term\n",
    "    V = np.zeros((num_subsets, M))\n",
    "\n",
    "    # Populate X, V, and weights based on subsets\n",
    "    weights = np.zeros(num_subsets)\n",
    "    for i, s in enumerate(subsets):\n",
    "        s = list(s)\n",
    "        V[i, :] = x  # Start with the full instance\n",
    "        \n",
    "        # Replace excluded features with a random far point sample\n",
    "        if len(s) < M:\n",
    "            far_sample = farthest_points[np.random.choice(k)]  # Randomly sample from far points\n",
    "            V[i, [j for j in range(M) if j not in s]] = far_sample[[j for j in range(M) if j not in s]]\n",
    "        \n",
    "        # Set included features\n",
    "        X[i, s] = 1\n",
    "        weights[i] = shapley_kernel(M, len(s))\n",
    "\n",
    "    # Normalize weights for numerical stability\n",
    "    weights /= np.sum(weights)\n",
    "\n",
    "    # Model predictions for perturbed inputs\n",
    "    y = f(V)\n",
    "\n",
    "    # Weighted least-squares regression\n",
    "    wsq = np.sqrt(weights)\n",
    "    result = np.linalg.lstsq(wsq[:, None] * X, wsq * y, rcond=None)[0]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Define the prediction function\n",
    "def prediction_function(X):\n",
    "    return regressor.predict(X)\n",
    "\n",
    "# Number of features\n",
    "M = X_train.shape[1]\n",
    "\n",
    "# Step 6: Compute SHAP values and time the computation\n",
    "\n",
    "# TreeExplainer SHAP\n",
    "start_time = time.time()\n",
    "tree_explainer = shap.TreeExplainer(regressor)\n",
    "tree_shap_values = tree_explainer.shap_values(test_instance.reshape(1, -1))[0]\n",
    "tree_baseline = float(tree_explainer.expected_value)  # Ensure scalar\n",
    "tree_time = time.time() - start_time\n",
    "\n",
    "# KernelExplainer SHAP\n",
    "start_time = time.time()\n",
    "kernel_explainer = shap.KernelExplainer(prediction_function, X_train)\n",
    "kernel_shap_values = kernel_explainer.shap_values(test_instance.reshape(1, -1))[0]\n",
    "kernel_baseline = float(kernel_explainer.expected_value)  # Ensure scalar\n",
    "kernel_time = time.time() - start_time\n",
    "\n",
    "# Mean Kernel SHAP\n",
    "start_time = time.time()\n",
    "reference = np.mean(X_train, axis=0)  # Mean feature values\n",
    "mean_kernel_phi = mean_kernel_shap(prediction_function, test_instance, reference, M)\n",
    "mean_baseline = float(mean_kernel_phi[-1])  # Baseline (intercept), ensure scalar\n",
    "mean_kernel_shap_values = mean_kernel_phi[:-1]  # SHAP values for features\n",
    "mean_kernel_time = time.time() - start_time\n",
    "\n",
    "# Kernel SHAP with farthest points\n",
    "start_time = time.time()\n",
    "num_samples = None  # Exact computation\n",
    "k = 100  # Number of farthest points to use\n",
    "far_kernel_phi = kernel_shap_with_far_points(prediction_function, test_instance, X_train, M, num_samples=num_samples, k=k)\n",
    "far_baseline = float(far_kernel_phi[-1])  # Baseline (intercept), ensure scalar\n",
    "far_kernel_shap_values = far_kernel_phi[:-1]  # SHAP values for features\n",
    "far_kernel_time = time.time() - start_time\n",
    "\n",
    "# Step 7: Display Results\n",
    "results = pd.DataFrame({\n",
    "    \"Feature\": [f\"Feature {i+1}\" for i in range(X.shape[1])],\n",
    "    \"TreeExplainer SHAP\": np.round(tree_shap_values, 4),\n",
    "    \"KernelExplainer SHAP\": np.round(kernel_shap_values, 4),\n",
    "    \"MeanKernel SHAP\": np.round(mean_kernel_shap_values, 4),\n",
    "    \"FarKernel SHAP\": np.round(far_kernel_shap_values, 4),\n",
    "})\n",
    "\n",
    "# Append baseline, sum, and computation time\n",
    "results = pd.concat([\n",
    "    results,\n",
    "    pd.DataFrame([\n",
    "        {\"Feature\": \"Baseline\", \"TreeExplainer SHAP\": np.round(tree_baseline, 4),\n",
    "         \"KernelExplainer SHAP\": np.round(kernel_baseline, 4),\n",
    "         \"MeanKernel SHAP\": np.round(mean_baseline, 4),\n",
    "         \"FarKernel SHAP\": np.round(far_baseline, 4)},\n",
    "        {\"Feature\": \"Sum\", \"TreeExplainer SHAP\": np.round(np.sum(tree_shap_values) + tree_baseline, 4),\n",
    "         \"KernelExplainer SHAP\": np.round(np.sum(kernel_shap_values) + kernel_baseline, 4),\n",
    "         \"MeanKernel SHAP\": np.round(np.sum(mean_kernel_shap_values) + mean_baseline, 4),\n",
    "         \"FarKernel SHAP\": np.round(np.sum(far_kernel_shap_values) + far_baseline, 4)},\n",
    "        {\"Feature\": \"Computation Time (s)\", \"TreeExplainer SHAP\": np.round(tree_time, 4),\n",
    "         \"KernelExplainer SHAP\": np.round(kernel_time, 4),\n",
    "         \"MeanKernel SHAP\": np.round(mean_kernel_time, 4),\n",
    "         \"FarKernel SHAP\": np.round(far_kernel_time, 4)},\n",
    "    ])\n",
    "])\n",
    "\n",
    "# Print the results\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b3f9cc-3c92-4e9f-84f6-6979219c0ce5",
   "metadata": {},
   "source": [
    "# mean with sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5228db5e-4cfd-4b56-8277-92ae8ced5e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0b78db447f44169cec8e9a2f607c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>TreeExplainer SHAP</th>\n",
       "      <th>KernelExplainer SHAP</th>\n",
       "      <th>MeanKernel SHAP (auto)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feature 1</td>\n",
       "      <td>-43.7884</td>\n",
       "      <td>-43.7849</td>\n",
       "      <td>-26.0819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feature 2</td>\n",
       "      <td>-8.6410</td>\n",
       "      <td>-7.6429</td>\n",
       "      <td>-6.6619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feature 3</td>\n",
       "      <td>7.1345</td>\n",
       "      <td>11.2744</td>\n",
       "      <td>8.2919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feature 4</td>\n",
       "      <td>13.0327</td>\n",
       "      <td>6.0428</td>\n",
       "      <td>4.3561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feature 5</td>\n",
       "      <td>-62.4468</td>\n",
       "      <td>-56.7780</td>\n",
       "      <td>-61.5866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Feature 6</td>\n",
       "      <td>-0.8848</td>\n",
       "      <td>0.1220</td>\n",
       "      <td>-0.3457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Feature 7</td>\n",
       "      <td>1.8424</td>\n",
       "      <td>2.6715</td>\n",
       "      <td>1.5175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Feature 8</td>\n",
       "      <td>10.3981</td>\n",
       "      <td>19.8204</td>\n",
       "      <td>15.5358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Feature 9</td>\n",
       "      <td>8.5729</td>\n",
       "      <td>7.9822</td>\n",
       "      <td>4.8366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Feature 10</td>\n",
       "      <td>-28.4331</td>\n",
       "      <td>-20.5258</td>\n",
       "      <td>-28.8828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.1591</td>\n",
       "      <td>-22.2361</td>\n",
       "      <td>-14.0330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sum</td>\n",
       "      <td>-103.0543</td>\n",
       "      <td>-103.0543</td>\n",
       "      <td>-103.0541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Computation Time (s)</td>\n",
       "      <td>0.2660</td>\n",
       "      <td>2.1375</td>\n",
       "      <td>0.0871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prediction (Test Instance)</td>\n",
       "      <td>-103.0543</td>\n",
       "      <td>-103.0543</td>\n",
       "      <td>-103.0543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Feature  TreeExplainer SHAP  KernelExplainer SHAP  \\\n",
       "0                   Feature 1            -43.7884              -43.7849   \n",
       "1                   Feature 2             -8.6410               -7.6429   \n",
       "2                   Feature 3              7.1345               11.2744   \n",
       "3                   Feature 4             13.0327                6.0428   \n",
       "4                   Feature 5            -62.4468              -56.7780   \n",
       "5                   Feature 6             -0.8848                0.1220   \n",
       "6                   Feature 7              1.8424                2.6715   \n",
       "7                   Feature 8             10.3981               19.8204   \n",
       "8                   Feature 9              8.5729                7.9822   \n",
       "9                  Feature 10            -28.4331              -20.5258   \n",
       "0                    Baseline              0.1591              -22.2361   \n",
       "1                         Sum           -103.0543             -103.0543   \n",
       "2        Computation Time (s)              0.2660                2.1375   \n",
       "3  Prediction (Test Instance)           -103.0543             -103.0543   \n",
       "\n",
       "   MeanKernel SHAP (auto)  \n",
       "0                -26.0819  \n",
       "1                 -6.6619  \n",
       "2                  8.2919  \n",
       "3                  4.3561  \n",
       "4                -61.5866  \n",
       "5                 -0.3457  \n",
       "6                  1.5175  \n",
       "7                 15.5358  \n",
       "8                  4.8366  \n",
       "9                -28.8828  \n",
       "0                -14.0330  \n",
       "1               -103.0541  \n",
       "2                  0.0871  \n",
       "3               -103.0543  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "import scipy.special\n",
    "import time\n",
    "import shap\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Step 1: Generate a simple regression dataset\n",
    "X, y = make_regression(n_samples=10000, n_features=10, noise=0.5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Train a RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Select the first test instance for explanation\n",
    "test_instance = X_test[0]\n",
    "\n",
    "# Shapley kernel weight calculation\n",
    "def shapley_kernel(M, s):\n",
    "    \"\"\"\n",
    "    Calculate the Shapley kernel weight for a subset of size s with M total features.\n",
    "\n",
    "    Parameters:\n",
    "    - M: Total number of features.\n",
    "    - s: Size of the subset.\n",
    "\n",
    "    Returns:\n",
    "    - The kernel weight for the subset size.\n",
    "    \"\"\"\n",
    "    if s == 0 or s == M:\n",
    "        return 10000  # Large constant for numerical stability\n",
    "    return (M - 1) / (scipy.special.binom(M, s) * s * (M - s))\n",
    "\n",
    "# Mean Kernel SHAP function (Exact computation)\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "def mean_kernel_shap_with_constraint(f, x, reference, M, nsamples=\"auto\"):\n",
    "    r\"\"\"\n",
    "    Kernel SHAP with additive efficiency constraint and kernel weights.\n",
    "    \n",
    "    Parameters:\n",
    "    - f: The model function to explain.\n",
    "    - x: Instance to explain (1D array of feature values).\n",
    "    - reference: Reference value for each feature (1D array, usually the mean of the dataset).\n",
    "    - M: Number of features.\n",
    "    - nsamples: Number of samples for the sampling method. \n",
    "                If \"auto\", uses nsamples = min(2 * M + 2048, 2^M).\n",
    "    \n",
    "    Returns:\n",
    "    - shap_values: Shapley values (1D array of size M).\n",
    "    - baseline: The baseline value (\\phi_0).\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Calculate the number of samples\n",
    "    if nsamples == \"auto\":\n",
    "        nsamples = min(2 * M + 2048, 2 ** M)\n",
    "\n",
    "    # Step 2: Sampling subsets with Shapley kernel weights\n",
    "    np.random.seed(0)\n",
    "    subsets = []  # List to store sampled subsets\n",
    "    weights = []  # List to store corresponding weights\n",
    "\n",
    "    for _ in range(nsamples):\n",
    "        # Generate a random binary mask representing a subset\n",
    "        subset = np.random.choice([0, 1], size=M, p=[0.5, 0.5])  # Random binary mask\n",
    "        subset_indices = np.where(subset == 1)[0]\n",
    "        subsets.append(subset_indices)\n",
    "        \n",
    "        # Compute Shapley kernel weight for the subset\n",
    "        s_size = len(subset_indices)\n",
    "        if s_size == 0 or s_size == M:\n",
    "            weight = 10000  # Large constant for stability\n",
    "        else:\n",
    "            weight = (M - 1) / (scipy.special.binom(M, s_size) * s_size * (M - s_size))\n",
    "        weights.append(weight)\n",
    "\n",
    "    # Normalize weights\n",
    "    weights = np.array(weights)\n",
    "    weights /= np.sum(weights)\n",
    "\n",
    "    # Number of subsets\n",
    "    num_subsets = len(subsets)\n",
    "\n",
    "    # Step 3: Initialize matrices for regression\n",
    "    X = np.zeros((num_subsets, M))  # Design matrix\n",
    "    V = np.zeros((num_subsets, M))  # Perturbed input data matrix\n",
    "\n",
    "    # Prepare feature subsets\n",
    "    for i, s in enumerate(subsets):\n",
    "        V[i, :] = reference  # Start with reference values\n",
    "        V[i, s] = x[s]       # Replace selected features with values from `x`\n",
    "        X[i, s] = 1          # Set feature presence in the design matrix\n",
    "\n",
    "    # Ensure `reference` and `x` are reshaped to (1, M) before passing to `f`\n",
    "    reference = reference.reshape(1, -1)\n",
    "    x = x.reshape(1, -1)\n",
    "\n",
    "    # Step 4: Evaluate the model on the sampled feature subsets\n",
    "    y = f(V) - f(reference)  # Centered outputs: v_x(s) - v_x(0)\n",
    "\n",
    "    # Step 5: Add efficiency constraint row to X and y\n",
    "    # Add a row to enforce the constraint: sum(_i) = v_x(1) - v_x(0)\n",
    "    efficiency_row = np.ones((1, M))  # Row of ones\n",
    "    X = np.vstack([X, efficiency_row])  # Append to the design matrix\n",
    "    y = np.append(y, f(x) - f(reference))  # Append the efficiency constraint to outputs\n",
    "\n",
    "    # Add corresponding weight for the efficiency constraint\n",
    "    weights = np.append(weights, 1.0)  # Assign unit weight to the efficiency constraint\n",
    "\n",
    "    # Step 6: Weighted least squares regression\n",
    "    # Compute weighted least squares: Minimize  (w_i * (y_i - X_i^T * )^2)\n",
    "    wsq = np.sqrt(weights)  # Square root of weights\n",
    "    result = np.linalg.lstsq(wsq[:, None] * X, wsq * y, rcond=None)[0]  # Solve for SHAP values\n",
    "\n",
    "    # Step 7: Return results\n",
    "    return result, f(reference).flatten()\n",
    "\n",
    "\n",
    "# Define the prediction function\n",
    "def prediction_function(X):\n",
    "    return regressor.predict(X)\n",
    "\n",
    "# Number of features\n",
    "M = X_train.shape[1]\n",
    "\n",
    "# Step 6: Compute SHAP values and time the computation\n",
    "\n",
    "# TreeExplainer SHAP\n",
    "start_time = time.time()\n",
    "tree_explainer = shap.TreeExplainer(regressor)\n",
    "tree_shap_values = tree_explainer.shap_values(test_instance.reshape(1, -1))[0]\n",
    "tree_baseline = float(tree_explainer.expected_value[0])  # Extract scalar from array\n",
    "tree_time = time.time() - start_time\n",
    "\n",
    "# KernelExplainer SHAP\n",
    "start_time = time.time()\n",
    "background = shap.sample(X_train, 100)  # Summarize the background to 100 samples\n",
    "kernel_explainer = shap.KernelExplainer(prediction_function, background)\n",
    "kernel_shap_values = kernel_explainer.shap_values(test_instance.reshape(1, -1))[0]\n",
    "kernel_baseline = float(kernel_explainer.expected_value)  # Ensure scalar\n",
    "kernel_time = time.time() - start_time\n",
    "\n",
    "# Mean Kernel SHAP (Auto)\n",
    "start_time = time.time()\n",
    "reference = np.mean(X_train, axis=0)  # Mean feature values\n",
    "mean_kernel_phi, mean_baseline = mean_kernel_shap_with_constraint(prediction_function, test_instance, reference, M)\n",
    "mean_kernel_shap_values = mean_kernel_phi  # SHAP values for features\n",
    "mean_kernel_time = time.time() - start_time\n",
    "\n",
    "# Compute the prediction of the test_instance\n",
    "test_instance_prediction = prediction_function(test_instance.reshape(1, -1))[0]\n",
    "\n",
    "# Step 6: Display Results\n",
    "results = pd.DataFrame({\n",
    "    \"Feature\": [f\"Feature {i+1}\" for i in range(X.shape[1])],\n",
    "    \"TreeExplainer SHAP\": np.round(tree_shap_values, 4),\n",
    "    \"KernelExplainer SHAP\": np.round(kernel_shap_values, 4),\n",
    "    \"MeanKernel SHAP (auto)\": np.round(mean_kernel_shap_values, 4),\n",
    "})\n",
    "\n",
    "# Append baseline, sum, computation time, and prediction\n",
    "results = pd.concat([\n",
    "    results,\n",
    "    pd.DataFrame([\n",
    "        {\"Feature\": \"Baseline\", \"TreeExplainer SHAP\": np.round(tree_baseline, 4),\n",
    "         \"KernelExplainer SHAP\": np.round(kernel_baseline, 4),\n",
    "         \"MeanKernel SHAP (auto)\": np.round(mean_baseline[0], 4)},\n",
    "        {\"Feature\": \"Sum\", \"TreeExplainer SHAP\": np.round(np.sum(tree_shap_values) + tree_baseline, 4),\n",
    "         \"KernelExplainer SHAP\": np.round(np.sum(kernel_shap_values) + kernel_baseline, 4),\n",
    "         \"MeanKernel SHAP (auto)\": np.round(np.sum(mean_kernel_shap_values) + mean_baseline[0], 4)},\n",
    "        {\"Feature\": \"Computation Time (s)\", \"TreeExplainer SHAP\": np.round(tree_time, 4),\n",
    "         \"KernelExplainer SHAP\": np.round(kernel_time, 4),\n",
    "         \"MeanKernel SHAP (auto)\": np.round(mean_kernel_time, 4)},\n",
    "        {\"Feature\": \"Prediction (Test Instance)\", \"TreeExplainer SHAP\": np.round(test_instance_prediction, 4),\n",
    "         \"KernelExplainer SHAP\": np.round(test_instance_prediction, 4), \"MeanKernel SHAP (auto)\": np.round(test_instance_prediction, 4)}\n",
    "    ])\n",
    "])\n",
    "\n",
    "\n",
    "# Print the results\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
